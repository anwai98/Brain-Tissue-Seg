{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F82NZ4gaeLQ5"
      },
      "source": [
        "For reference : https://medium.com/miccai-educational-initiative/nnu-net-the-no-new-unet-for-automatic-segmentation-8d655f3f6d2a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF9gX8Fuq_K8",
        "outputId": "9bef2b59-af94-4c28-a415-424d2d8df0a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan 30 11:28:24 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjmKuaXtpDBt"
      },
      "source": [
        "## Download NNUnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "id": "IPXGsf_tXPWK",
        "outputId": "d289de7d-35a3-46e3-cff0-2cb74d32dd6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nnunet in /usr/local/lib/python3.7/dist-packages (1.7.0)\n",
            "Requirement already satisfied: torch>=1.6.0a in /usr/local/lib/python3.7/dist-packages (from nnunet) (1.10.0+cu111)\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.7/dist-packages (from nnunet) (2021.11.2)\n",
            "Requirement already satisfied: medpy in /usr/local/lib/python3.7/dist-packages (from nnunet) (0.4.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (from nnunet) (3.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nnunet) (4.62.3)\n",
            "Requirement already satisfied: scikit-image>=0.14 in /usr/local/lib/python3.7/dist-packages (from nnunet) (0.18.3)\n",
            "Requirement already satisfied: batchgenerators>=0.23 in /usr/local/lib/python3.7/dist-packages (from nnunet) (0.23)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nnunet) (1.19.5)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from nnunet) (0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from nnunet) (1.1.5)\n",
            "Requirement already satisfied: dicom2nifti in /usr/local/lib/python3.7/dist-packages (from nnunet) (2.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from nnunet) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from nnunet) (2.23.0)\n",
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.7/dist-packages (from nnunet) (2.1.1)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.7/dist-packages (from batchgenerators>=0.23->nnunet) (3.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from batchgenerators>=0.23->nnunet) (0.16.0)\n",
            "Requirement already satisfied: unittest2 in /usr/local/lib/python3.7/dist-packages (from batchgenerators>=0.23->nnunet) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from batchgenerators>=0.23->nnunet) (1.0.2)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from batchgenerators>=0.23->nnunet) (7.1.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14->nnunet) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14->nnunet) (2.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14->nnunet) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14->nnunet) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->nnunet) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->nnunet) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->nnunet) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->nnunet) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->nnunet) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0a->nnunet) (3.10.0.2)\n",
            "Requirement already satisfied: pydicom>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from dicom2nifti->nnunet) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->nnunet) (2018.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->nnunet) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->nnunet) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->nnunet) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->nnunet) (3.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->batchgenerators>=0.23->nnunet) (1.1.0)\n",
            "Collecting argparse\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: traceback2 in /usr/local/lib/python3.7/dist-packages (from unittest2->batchgenerators>=0.23->nnunet) (1.4.0)\n",
            "Requirement already satisfied: linecache2 in /usr/local/lib/python3.7/dist-packages (from traceback2->unittest2->batchgenerators>=0.23->nnunet) (1.0.0)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install nnunet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-mF1QkPpJ3T"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btI97_sgVZgb"
      },
      "outputs": [],
      "source": [
        "from urllib import request\n",
        "import pathlib\n",
        "import zipfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP_Rvo3spM_e"
      },
      "source": [
        "## Setting the Environment and Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "eMmLGBHmjOqU",
        "outputId": "40d09617-a438-4ae1-b5f3-dc76f33941d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\\n!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\\n!apt-get update -qq 2>&1 > /dev/null\\n!apt-get -y install -qq google-drive-ocamlfuse fuse\\nfrom google.colab import auth\\nauth.authenticate_user()\\nfrom oauth2client.client import GoogleCredentials\\ncreds = GoogleCredentials.get_application_default()\\nimport getpass\\n!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\\nvcode = getpass.getpass()\\n!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\\n%cd /content\\n!mkdir drive\\n%cd drive\\n!mkdir MyDrive\\n%cd ..\\n%cd ..\\n!google-drive-ocamlfuse /content/drive/MyDrive'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "%cd /content\n",
        "!mkdir drive\n",
        "%cd drive\n",
        "!mkdir MyDrive\n",
        "%cd ..\n",
        "%cd ..\n",
        "!google-drive-ocamlfuse /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgeBAglDPCnz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "004c0160-5f54-4615-b1ce-51c68eae6188"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"from google.colab import drive, auth, output\\nauth.authenticate_user()\\ndrive.mount('/content/drive', force_remount=True)\""
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "\"\"\"from google.colab import drive, auth, output\n",
        "auth.authenticate_user()\n",
        "drive.mount('/content/drive', force_remount=True)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3dkzefcVfkM",
        "outputId": "9dcfe7d1-b3d8-4f2b-eaec-3d6bbc914efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task701_IBSR\n"
          ]
        }
      ],
      "source": [
        "url_path = '/content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/Task701_IBSR.zip'\n",
        "filename = url_path.split('/')[-1][:12]\n",
        "print(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoLdP5jzVwIk"
      },
      "outputs": [],
      "source": [
        "#if not pathlib.Path(filename).exists():\n",
        "#    request.urlretrieve(url, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9X-JEF5VwYU"
      },
      "outputs": [],
      "source": [
        "#here = pathlib.Path('.').resolve()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DiI2dPBV8n7"
      },
      "outputs": [],
      "source": [
        "base_dir = '/content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/'\n",
        "nnUNet_raw_data_base = os.path.join(base_dir, 'nnUNet_raw_data_base')\n",
        "nnUNet_preprocessed = os.path.join(base_dir, 'nnUNet_preprocessed')\n",
        "results_folder = os.path.join(base_dir, 'results')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VhaRN2nXkKc",
        "outputId": "207151fb-c30c-4929-d462-645327f16851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/nnUNet_raw_data_base/nnUNet_raw_data\n"
          ]
        }
      ],
      "source": [
        "#raw_data_dir = nnUNet_raw_data_base.joinpath('nnUNet_raw_data')\n",
        "raw_data_dir = os.path.join(nnUNet_raw_data_base, 'nnUNet_raw_data')\n",
        "print(raw_data_dir)\n",
        "#raw_data_dir.mkdir(exist_ok=True, parents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS6_iY3hXuFs"
      },
      "outputs": [],
      "source": [
        "# Already extracted\n",
        "#zip_ref = zipfile.ZipFile(url_path, 'r')\n",
        "#zip_ref.extractall(raw_data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dt8CM7yaWjSM"
      },
      "outputs": [],
      "source": [
        "os.environ[\"nnUNet_raw_data_base\"] = str(nnUNet_raw_data_base)\n",
        "os.environ[\"nnUNet_preprocessed\"] = str(nnUNet_preprocessed)\n",
        "os.environ[\"RESULTS_FOLDER\"] = str(results_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "E91ocQxmx_WL",
        "outputId": "fe98b535-1423-4c75-d198-63016692bd0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/results'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "results_folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdhyHUZLpTJu"
      },
      "source": [
        "## Data Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35uSX0aPWu-c"
      },
      "outputs": [],
      "source": [
        "!nnUNet_plan_and_preprocess -t 701 --verify_dataset_integrity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE-MMz54pYLm"
      },
      "source": [
        "## Training\n",
        "\n",
        "* To train - nnUNet_train TRAINER_CLASS_NAME TASK_NAME_OR_ID FOLD\n",
        "* To resume - nnUNet_train TRAINER_CLASS_NAME TASK_NAME_OR_ID FOLD -c (just add -c to the training command)\n",
        "* TRAINER_CLASS_NAME - 2d, 3d_fullres, 3d_lowres, 3d_cascade_fullres\n",
        "* Everything will be stored in the results folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D3OZgqCXJw9",
        "outputId": "1ce40b01-293d-4f1a-cc3b-f6a15130f4a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Please cite the following paper when using nnUNet:\n",
            "\n",
            "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
            "\n",
            "\n",
            "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
            "\n",
            "###############################################\n",
            "I am running the following nnUNet: 2d\n",
            "My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>\n",
            "For that I will be using the following configuration:\n",
            "num_classes:  3\n",
            "modalities:  {0: 'T1-w'}\n",
            "use_mask_for_norm OrderedDict([(0, True)])\n",
            "keep_only_largest_region None\n",
            "min_region_size_per_class None\n",
            "min_size_per_class None\n",
            "normalization_schemes OrderedDict([(0, 'nonCT')])\n",
            "stages...\n",
            "\n",
            "stage:  0\n",
            "{'batch_size': 45, 'num_pool_per_axis': [5, 5], 'patch_size': array([160, 160]), 'median_patient_size_in_voxels': array([115, 139, 144]), 'current_spacing': array([1.5   , 0.9375, 0.9375]), 'original_spacing': array([1.5   , 0.9375, 0.9375]), 'pool_op_kernel_sizes': [[2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'do_dummy_2D_data_aug': False}\n",
            "\n",
            "I am using stage 0 from these plans\n",
            "I am using batch dice + CE loss\n",
            "\n",
            "I am using data from this folder:  /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/nnUNet_preprocessed/Task701_IBSR/nnUNetData_plans_v2.1_2D\n",
            "###############################################\n",
            "loading dataset\n",
            "loading all case properties\n",
            "2022-01-25 23:53:18.221606: Using splits from existing split file: /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/nnUNet_preprocessed/Task701_IBSR/splits_final.pkl\n",
            "2022-01-25 23:53:23.886718: The split file contains 5 splits.\n",
            "2022-01-25 23:53:24.850377: Desired fold for training: 3\n",
            "2022-01-25 23:53:26.878523: This split has 8 training and 2 validation cases.\n",
            "unpacking dataset\n",
            "done\n",
            "2022-01-25 23:53:46.982590: loading checkpoint /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/results/nnUNet/2d/Task701_IBSR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_3/model_latest.model train= True\n",
            "2022-01-25 23:53:49.654878: lr: 0.001259\n",
            "using pin_memory on device 0\n",
            "using pin_memory on device 0\n",
            "2022-01-25 23:54:23.690041: Unable to plot network architecture:\n",
            "2022-01-25 23:54:24.914407: No module named 'hiddenlayer'\n",
            "2022-01-25 23:54:26.186329: \n",
            "printing the network instead:\n",
            "\n",
            "2022-01-25 23:54:27.212887: Generic_UNet(\n",
            "  (conv_blocks_localization): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): StackedConvLayers(\n",
            "        (blocks): Sequential(\n",
            "          (0): ConvDropoutNormNonlin(\n",
            "            (conv): Conv2d(960, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (instnorm): InstanceNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): StackedConvLayers(\n",
            "        (blocks): Sequential(\n",
            "          (0): ConvDropoutNormNonlin(\n",
            "            (conv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (instnorm): InstanceNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): StackedConvLayers(\n",
            "        (blocks): Sequential(\n",
            "          (0): ConvDropoutNormNonlin(\n",
            "            (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (instnorm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): StackedConvLayers(\n",
            "        (blocks): Sequential(\n",
            "          (0): ConvDropoutNormNonlin(\n",
            "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (instnorm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): StackedConvLayers(\n",
            "        (blocks): Sequential(\n",
            "          (0): ConvDropoutNormNonlin(\n",
            "            (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (instnorm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): StackedConvLayers(\n",
            "        (blocks): Sequential(\n",
            "          (0): ConvDropoutNormNonlin(\n",
            "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (instnorm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): StackedConvLayers(\n",
            "        (blocks): Sequential(\n",
            "          (0): ConvDropoutNormNonlin(\n",
            "            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (instnorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): StackedConvLayers(\n",
            "        (blocks): Sequential(\n",
            "          (0): ConvDropoutNormNonlin(\n",
            "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (instnorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (4): Sequential(\n",
            "      (0): StackedConvLayers(\n",
            "        (blocks): Sequential(\n",
            "          (0): ConvDropoutNormNonlin(\n",
            "            (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (instnorm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): StackedConvLayers(\n",
            "        (blocks): Sequential(\n",
            "          (0): ConvDropoutNormNonlin(\n",
            "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (instnorm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv_blocks_context): ModuleList(\n",
            "    (0): StackedConvLayers(\n",
            "      (blocks): Sequential(\n",
            "        (0): ConvDropoutNormNonlin(\n",
            "          (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (instnorm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        )\n",
            "        (1): ConvDropoutNormNonlin(\n",
            "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (instnorm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): StackedConvLayers(\n",
            "      (blocks): Sequential(\n",
            "        (0): ConvDropoutNormNonlin(\n",
            "          (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (instnorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        )\n",
            "        (1): ConvDropoutNormNonlin(\n",
            "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (instnorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): StackedConvLayers(\n",
            "      (blocks): Sequential(\n",
            "        (0): ConvDropoutNormNonlin(\n",
            "          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (instnorm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        )\n",
            "        (1): ConvDropoutNormNonlin(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (instnorm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (3): StackedConvLayers(\n",
            "      (blocks): Sequential(\n",
            "        (0): ConvDropoutNormNonlin(\n",
            "          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (instnorm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        )\n",
            "        (1): ConvDropoutNormNonlin(\n",
            "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (instnorm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (4): StackedConvLayers(\n",
            "      (blocks): Sequential(\n",
            "        (0): ConvDropoutNormNonlin(\n",
            "          (conv): Conv2d(256, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (instnorm): InstanceNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        )\n",
            "        (1): ConvDropoutNormNonlin(\n",
            "          (conv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (instnorm): InstanceNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): StackedConvLayers(\n",
            "        (blocks): Sequential(\n",
            "          (0): ConvDropoutNormNonlin(\n",
            "            (conv): Conv2d(480, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            (instnorm): InstanceNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): StackedConvLayers(\n",
            "        (blocks): Sequential(\n",
            "          (0): ConvDropoutNormNonlin(\n",
            "            (conv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (instnorm): InstanceNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (td): ModuleList()\n",
            "  (tu): ModuleList(\n",
            "    (0): ConvTranspose2d(480, 480, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
            "    (1): ConvTranspose2d(480, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
            "    (2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
            "    (3): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
            "    (4): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
            "  )\n",
            "  (seg_outputs): ModuleList(\n",
            "    (0): Conv2d(480, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (2): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (3): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (4): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "  )\n",
            ")\n",
            "2022-01-25 23:54:28.465596: \n",
            "\n",
            "2022-01-25 23:54:30.650255: \n",
            "epoch:  900\n",
            "2022-01-25 23:55:52.974755: train loss : -0.9527\n",
            "2022-01-25 23:56:14.496839: validation loss: -0.8012\n",
            "2022-01-25 23:56:15.512316: Average global foreground Dice: [0.8616, 0.9578, 0.9387]\n",
            "2022-01-25 23:56:17.541477: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-25 23:56:22.981150: lr: 0.001248\n",
            "2022-01-25 23:56:24.228494: This epoch took 112.555753 s\n",
            "\n",
            "2022-01-25 23:56:26.319398: \n",
            "epoch:  901\n",
            "2022-01-25 23:57:48.044128: train loss : -0.9520\n",
            "2022-01-25 23:58:08.281439: validation loss: -0.8115\n",
            "2022-01-25 23:58:09.446238: Average global foreground Dice: [0.8674, 0.958, 0.9395]\n",
            "2022-01-25 23:58:11.428445: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-25 23:58:16.368299: lr: 0.001236\n",
            "2022-01-25 23:58:17.377232: This epoch took 108.614089 s\n",
            "\n",
            "2022-01-25 23:58:19.236536: \n",
            "epoch:  902\n",
            "2022-01-25 23:59:39.161120: train loss : -0.9523\n",
            "2022-01-26 00:00:00.509137: validation loss: -0.8080\n",
            "2022-01-26 00:00:01.647212: Average global foreground Dice: [0.8654, 0.958, 0.9395]\n",
            "2022-01-26 00:00:03.852791: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:00:07.882547: lr: 0.001225\n",
            "2022-01-26 00:00:08.964441: This epoch took 107.853182 s\n",
            "\n",
            "2022-01-26 00:00:11.045876: \n",
            "epoch:  903\n",
            "2022-01-26 00:01:32.549050: train loss : -0.9523\n",
            "2022-01-26 00:01:52.382456: validation loss: -0.8097\n",
            "2022-01-26 00:01:53.492966: Average global foreground Dice: [0.8693, 0.9582, 0.9394]\n",
            "2022-01-26 00:01:55.605111: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:02:01.069351: lr: 0.001214\n",
            "2022-01-26 00:02:02.162930: This epoch took 109.380842 s\n",
            "\n",
            "2022-01-26 00:02:04.498183: \n",
            "epoch:  904\n",
            "2022-01-26 00:03:25.921077: train loss : -0.9523\n",
            "2022-01-26 00:03:46.349954: validation loss: -0.8059\n",
            "2022-01-26 00:03:47.453401: Average global foreground Dice: [0.8673, 0.9578, 0.9386]\n",
            "2022-01-26 00:03:49.670001: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:03:53.900026: lr: 0.001202\n",
            "2022-01-26 00:03:54.877227: This epoch took 108.120358 s\n",
            "\n",
            "2022-01-26 00:03:56.018688: \n",
            "epoch:  905\n",
            "2022-01-26 00:05:17.954008: train loss : -0.9521\n",
            "2022-01-26 00:05:38.303225: validation loss: -0.8091\n",
            "2022-01-26 00:05:39.380949: Average global foreground Dice: [0.8651, 0.9582, 0.9396]\n",
            "2022-01-26 00:05:40.384754: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:05:45.590570: lr: 0.001191\n",
            "2022-01-26 00:05:46.709731: This epoch took 108.603491 s\n",
            "\n",
            "2022-01-26 00:05:48.723629: \n",
            "epoch:  906\n",
            "2022-01-26 00:07:10.545576: train loss : -0.9529\n",
            "2022-01-26 00:07:30.142610: validation loss: -0.8069\n",
            "2022-01-26 00:07:31.196953: Average global foreground Dice: [0.8628, 0.9586, 0.9407]\n",
            "2022-01-26 00:07:33.160238: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:07:38.144671: lr: 0.001179\n",
            "2022-01-26 00:07:39.273869: This epoch took 108.575905 s\n",
            "\n",
            "2022-01-26 00:07:40.358471: \n",
            "epoch:  907\n",
            "2022-01-26 00:09:02.312942: train loss : -0.9516\n",
            "2022-01-26 00:09:22.100363: validation loss: -0.8070\n",
            "2022-01-26 00:09:23.469393: Average global foreground Dice: [0.8642, 0.9583, 0.9402]\n",
            "2022-01-26 00:09:25.569187: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:09:30.558258: lr: 0.001168\n",
            "2022-01-26 00:09:31.690157: This epoch took 109.413185 s\n",
            "\n",
            "2022-01-26 00:09:32.808592: \n",
            "epoch:  908\n",
            "2022-01-26 00:10:54.365774: train loss : -0.9524\n",
            "2022-01-26 00:11:14.312392: validation loss: -0.8157\n",
            "2022-01-26 00:11:15.618562: Average global foreground Dice: [0.8663, 0.959, 0.9399]\n",
            "2022-01-26 00:11:17.914135: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:11:23.009036: lr: 0.001156\n",
            "2022-01-26 00:11:24.206897: This epoch took 109.515105 s\n",
            "\n",
            "2022-01-26 00:11:26.106884: \n",
            "epoch:  909\n",
            "2022-01-26 00:12:47.358167: train loss : -0.9530\n",
            "2022-01-26 00:13:07.409140: validation loss: -0.8107\n",
            "2022-01-26 00:13:08.551898: Average global foreground Dice: [0.8682, 0.9584, 0.9397]\n",
            "2022-01-26 00:13:10.769657: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:13:15.533386: lr: 0.001145\n",
            "2022-01-26 00:13:16.478049: This epoch took 108.246015 s\n",
            "\n",
            "2022-01-26 00:13:18.617506: \n",
            "epoch:  910\n",
            "2022-01-26 00:14:38.639280: train loss : -0.9536\n",
            "2022-01-26 00:14:59.287735: validation loss: -0.8111\n",
            "2022-01-26 00:15:00.518708: Average global foreground Dice: [0.867, 0.9589, 0.94]\n",
            "2022-01-26 00:15:02.626197: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:15:07.332826: lr: 0.001134\n",
            "2022-01-26 00:15:08.468803: This epoch took 107.806410 s\n",
            "\n",
            "2022-01-26 00:15:10.602808: \n",
            "epoch:  911\n",
            "2022-01-26 00:16:31.803253: train loss : -0.9523\n",
            "2022-01-26 00:16:52.928023: validation loss: -0.8078\n",
            "2022-01-26 00:16:54.150973: Average global foreground Dice: [0.8647, 0.9582, 0.9402]\n",
            "2022-01-26 00:16:56.262663: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:17:00.444142: lr: 0.001122\n",
            "2022-01-26 00:17:02.072794: This epoch took 109.510096 s\n",
            "\n",
            "2022-01-26 00:17:04.041361: \n",
            "epoch:  912\n",
            "2022-01-26 00:18:24.791872: train loss : -0.9525\n",
            "2022-01-26 00:18:45.536756: validation loss: -0.8062\n",
            "2022-01-26 00:18:46.713064: Average global foreground Dice: [0.8645, 0.9578, 0.9392]\n",
            "2022-01-26 00:18:48.871165: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:18:53.966191: lr: 0.001111\n",
            "2022-01-26 00:18:55.488627: This epoch took 109.305779 s\n",
            "\n",
            "2022-01-26 00:18:58.251467: \n",
            "epoch:  913\n",
            "2022-01-26 00:20:19.087643: train loss : -0.9530\n",
            "2022-01-26 00:20:39.781060: validation loss: -0.8075\n",
            "2022-01-26 00:20:41.044472: Average global foreground Dice: [0.8677, 0.9583, 0.9399]\n",
            "2022-01-26 00:20:43.333365: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:20:46.423419: lr: 0.001099\n",
            "2022-01-26 00:20:47.671613: This epoch took 107.331467 s\n",
            "\n",
            "2022-01-26 00:20:50.122953: \n",
            "epoch:  914\n",
            "2022-01-26 00:22:12.109633: train loss : -0.9523\n",
            "2022-01-26 00:22:32.946703: validation loss: -0.8076\n",
            "2022-01-26 00:22:34.230009: Average global foreground Dice: [0.865, 0.9579, 0.9389]\n",
            "2022-01-26 00:22:36.516268: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:22:41.566070: lr: 0.001088\n",
            "2022-01-26 00:22:42.797598: This epoch took 110.428938 s\n",
            "\n",
            "2022-01-26 00:22:43.941466: \n",
            "epoch:  915\n",
            "2022-01-26 00:24:05.520380: train loss : -0.9525\n",
            "2022-01-26 00:24:25.835990: validation loss: -0.8071\n",
            "2022-01-26 00:24:27.153230: Average global foreground Dice: [0.8654, 0.9583, 0.9392]\n",
            "2022-01-26 00:24:29.362230: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:24:34.517174: lr: 0.001076\n",
            "2022-01-26 00:24:35.685719: This epoch took 109.151876 s\n",
            "\n",
            "2022-01-26 00:24:37.988301: \n",
            "epoch:  916\n",
            "2022-01-26 00:25:58.750589: train loss : -0.9528\n",
            "2022-01-26 00:26:19.488988: validation loss: -0.8095\n",
            "2022-01-26 00:26:20.744917: Average global foreground Dice: [0.8693, 0.9582, 0.9398]\n",
            "2022-01-26 00:26:22.871596: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:26:28.000424: lr: 0.001065\n",
            "2022-01-26 00:26:29.341440: This epoch took 108.960388 s\n",
            "\n",
            "2022-01-26 00:26:31.596470: \n",
            "epoch:  917\n",
            "2022-01-26 00:27:51.447016: train loss : -0.9525\n",
            "2022-01-26 00:28:12.330049: validation loss: -0.8106\n",
            "2022-01-26 00:28:13.497470: Average global foreground Dice: [0.8642, 0.9588, 0.9413]\n",
            "2022-01-26 00:28:15.572371: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:28:20.901812: lr: 0.001053\n",
            "2022-01-26 00:28:22.011377: This epoch took 109.264834 s\n",
            "\n",
            "2022-01-26 00:28:24.020066: \n",
            "epoch:  918\n",
            "2022-01-26 00:29:44.550885: train loss : -0.9523\n",
            "2022-01-26 00:30:06.002152: validation loss: -0.8065\n",
            "2022-01-26 00:30:07.268398: Average global foreground Dice: [0.8658, 0.9587, 0.9404]\n",
            "2022-01-26 00:30:09.718849: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:30:14.802985: lr: 0.001041\n",
            "2022-01-26 00:30:15.965719: This epoch took 109.589968 s\n",
            "\n",
            "2022-01-26 00:30:18.374971: \n",
            "epoch:  919\n",
            "2022-01-26 00:31:39.131976: train loss : -0.9528\n",
            "2022-01-26 00:31:59.642346: validation loss: -0.8058\n",
            "2022-01-26 00:32:00.880463: Average global foreground Dice: [0.8689, 0.9578, 0.939]\n",
            "2022-01-26 00:32:03.311150: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:32:07.343789: lr: 0.00103\n",
            "2022-01-26 00:32:08.538799: This epoch took 107.796012 s\n",
            "\n",
            "2022-01-26 00:32:10.790602: \n",
            "epoch:  920\n",
            "2022-01-26 00:33:31.823488: train loss : -0.9525\n",
            "2022-01-26 00:33:52.783715: validation loss: -0.8049\n",
            "2022-01-26 00:33:53.971009: Average global foreground Dice: [0.8669, 0.9577, 0.9391]\n",
            "2022-01-26 00:33:56.282199: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:34:01.518683: lr: 0.001018\n",
            "2022-01-26 00:34:02.773724: This epoch took 109.838699 s\n",
            "\n",
            "2022-01-26 00:34:05.256918: \n",
            "epoch:  921\n",
            "2022-01-26 00:35:27.597495: train loss : -0.9528\n",
            "2022-01-26 00:35:46.079264: validation loss: -0.8192\n",
            "2022-01-26 00:35:47.255378: Average global foreground Dice: [0.871, 0.9591, 0.9409]\n",
            "2022-01-26 00:35:49.529957: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:35:54.632643: lr: 0.001007\n",
            "2022-01-26 00:35:55.676865: This epoch took 108.842074 s\n",
            "\n",
            "2022-01-26 00:35:57.866473: \n",
            "epoch:  922\n",
            "2022-01-26 00:37:18.888384: train loss : -0.9524\n",
            "2022-01-26 00:37:38.347395: validation loss: -0.8061\n",
            "2022-01-26 00:37:39.634809: Average global foreground Dice: [0.8658, 0.9582, 0.9398]\n",
            "2022-01-26 00:37:41.699420: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:37:45.850761: lr: 0.000995\n",
            "2022-01-26 00:37:46.929934: This epoch took 108.002562 s\n",
            "\n",
            "2022-01-26 00:37:48.024969: \n",
            "epoch:  923\n",
            "2022-01-26 00:39:11.370697: train loss : -0.9529\n",
            "2022-01-26 00:39:30.054996: validation loss: -0.8074\n",
            "2022-01-26 00:39:31.311425: Average global foreground Dice: [0.8642, 0.9579, 0.9396]\n",
            "2022-01-26 00:39:33.584158: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:39:38.675412: lr: 0.000983\n",
            "2022-01-26 00:39:39.820018: This epoch took 109.639847 s\n",
            "\n",
            "2022-01-26 00:39:42.005594: \n",
            "epoch:  924\n",
            "2022-01-26 00:41:02.292857: train loss : -0.9528\n",
            "2022-01-26 00:41:22.922710: validation loss: -0.8090\n",
            "2022-01-26 00:41:24.048622: Average global foreground Dice: [0.8688, 0.9579, 0.9388]\n",
            "2022-01-26 00:41:25.383453: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:41:30.578312: lr: 0.000972\n",
            "2022-01-26 00:41:31.721612: This epoch took 108.480546 s\n",
            "\n",
            "2022-01-26 00:41:33.983440: \n",
            "epoch:  925\n",
            "2022-01-26 00:42:54.764357: train loss : -0.9530\n",
            "2022-01-26 00:43:15.352833: validation loss: -0.8068\n",
            "2022-01-26 00:43:16.627184: Average global foreground Dice: [0.8664, 0.9577, 0.939]\n",
            "2022-01-26 00:43:18.751292: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:43:24.018343: lr: 0.00096\n",
            "2022-01-26 00:43:25.106740: This epoch took 108.755498 s\n",
            "\n",
            "2022-01-26 00:43:27.278037: \n",
            "epoch:  926\n",
            "2022-01-26 00:44:48.624047: train loss : -0.9529\n",
            "2022-01-26 00:45:08.456128: validation loss: -0.8079\n",
            "2022-01-26 00:45:09.781246: Average global foreground Dice: [0.8622, 0.9585, 0.9406]\n",
            "2022-01-26 00:45:12.226368: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:45:17.271550: lr: 0.000948\n",
            "2022-01-26 00:45:18.367279: This epoch took 108.931979 s\n",
            "\n",
            "2022-01-26 00:45:20.528169: \n",
            "epoch:  927\n",
            "2022-01-26 00:46:41.489352: train loss : -0.9528\n",
            "2022-01-26 00:47:01.166528: validation loss: -0.8091\n",
            "2022-01-26 00:47:02.401993: Average global foreground Dice: [0.8632, 0.9586, 0.9397]\n",
            "2022-01-26 00:47:04.602278: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:47:10.117944: lr: 0.000937\n",
            "2022-01-26 00:47:11.507130: This epoch took 109.941267 s\n",
            "\n",
            "2022-01-26 00:47:13.878825: \n",
            "epoch:  928\n",
            "2022-01-26 00:48:35.209998: train loss : -0.9534\n",
            "2022-01-26 00:48:55.312000: validation loss: -0.8092\n",
            "2022-01-26 00:48:56.513000: Average global foreground Dice: [0.8684, 0.9583, 0.9394]\n",
            "2022-01-26 00:48:58.666694: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:49:02.197705: lr: 0.000925\n",
            "2022-01-26 00:49:03.500561: This epoch took 108.605630 s\n",
            "\n",
            "2022-01-26 00:49:05.455244: \n",
            "epoch:  929\n",
            "2022-01-26 00:50:27.894572: train loss : -0.9525\n",
            "2022-01-26 00:50:48.268478: validation loss: -0.8133\n",
            "2022-01-26 00:50:49.542611: Average global foreground Dice: [0.8728, 0.9585, 0.9404]\n",
            "2022-01-26 00:50:50.856761: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:50:57.191897: lr: 0.000913\n",
            "2022-01-26 00:50:58.343508: This epoch took 110.786066 s\n",
            "\n",
            "2022-01-26 00:51:00.601568: \n",
            "epoch:  930\n",
            "2022-01-26 00:52:22.455890: train loss : -0.9533\n",
            "2022-01-26 00:52:43.012917: validation loss: -0.8067\n",
            "2022-01-26 00:52:44.203823: Average global foreground Dice: [0.8689, 0.9579, 0.9385]\n",
            "2022-01-26 00:52:46.524899: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:52:51.314839: lr: 0.000901\n",
            "2022-01-26 00:52:52.461128: This epoch took 109.756603 s\n",
            "\n",
            "2022-01-26 00:52:54.768235: \n",
            "epoch:  931\n",
            "2022-01-26 00:54:13.959871: train loss : -0.9534\n",
            "2022-01-26 00:54:34.992379: validation loss: -0.8037\n",
            "2022-01-26 00:54:36.127954: Average global foreground Dice: [0.8663, 0.9576, 0.9386]\n",
            "2022-01-26 00:54:38.432385: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:54:42.710620: lr: 0.00089\n",
            "2022-01-26 00:54:43.781753: This epoch took 107.855826 s\n",
            "\n",
            "2022-01-26 00:54:45.632432: \n",
            "epoch:  932\n",
            "2022-01-26 00:56:06.853559: train loss : -0.9530\n",
            "2022-01-26 00:56:27.463103: validation loss: -0.8096\n",
            "2022-01-26 00:56:28.676058: Average global foreground Dice: [0.866, 0.9584, 0.9404]\n",
            "2022-01-26 00:56:29.905569: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:56:35.178415: lr: 0.000878\n",
            "2022-01-26 00:56:36.365658: This epoch took 108.464714 s\n",
            "\n",
            "2022-01-26 00:56:38.628579: \n",
            "epoch:  933\n",
            "2022-01-26 00:57:59.957583: train loss : -0.9533\n",
            "2022-01-26 00:58:19.495328: validation loss: -0.8055\n",
            "2022-01-26 00:58:20.863025: Average global foreground Dice: [0.8634, 0.9583, 0.9401]\n",
            "2022-01-26 00:58:22.088896: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 00:58:27.169066: lr: 0.000866\n",
            "2022-01-26 00:58:28.370324: This epoch took 108.545705 s\n",
            "\n",
            "2022-01-26 00:58:30.507893: \n",
            "epoch:  934\n",
            "2022-01-26 00:59:52.173637: train loss : -0.9537\n",
            "2022-01-26 01:00:12.266252: validation loss: -0.8110\n",
            "2022-01-26 01:00:13.501938: Average global foreground Dice: [0.8687, 0.9585, 0.94]\n",
            "2022-01-26 01:00:15.469141: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:00:20.453123: lr: 0.000854\n",
            "2022-01-26 01:00:21.606735: This epoch took 109.168585 s\n",
            "\n",
            "2022-01-26 01:00:23.957201: \n",
            "epoch:  935\n",
            "2022-01-26 01:01:42.819853: train loss : -0.9525\n",
            "2022-01-26 01:02:04.552768: validation loss: -0.8117\n",
            "2022-01-26 01:02:05.770990: Average global foreground Dice: [0.8684, 0.9581, 0.9393]\n",
            "2022-01-26 01:02:08.028051: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:02:13.628735: lr: 0.000842\n",
            "2022-01-26 01:02:15.034718: This epoch took 109.924003 s\n",
            "\n",
            "2022-01-26 01:02:16.202985: \n",
            "epoch:  936\n",
            "2022-01-26 01:03:37.759681: train loss : -0.9535\n",
            "2022-01-26 01:03:58.700244: validation loss: -0.8103\n",
            "2022-01-26 01:03:59.857182: Average global foreground Dice: [0.8697, 0.9588, 0.9402]\n",
            "2022-01-26 01:04:01.818299: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:04:06.569464: lr: 0.000831\n",
            "2022-01-26 01:04:07.767901: This epoch took 109.323386 s\n",
            "\n",
            "2022-01-26 01:04:10.199882: \n",
            "epoch:  937\n",
            "2022-01-26 01:05:32.290007: train loss : -0.9537\n",
            "2022-01-26 01:05:52.494045: validation loss: -0.8066\n",
            "2022-01-26 01:05:53.933062: Average global foreground Dice: [0.8691, 0.958, 0.9388]\n",
            "2022-01-26 01:05:55.289181: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:06:00.366402: lr: 0.000819\n",
            "2022-01-26 01:06:01.606771: This epoch took 109.171803 s\n",
            "\n",
            "2022-01-26 01:06:04.077539: \n",
            "epoch:  938\n",
            "2022-01-26 01:07:27.976112: train loss : -0.9535\n",
            "2022-01-26 01:07:47.023003: validation loss: -0.8131\n",
            "2022-01-26 01:07:48.315783: Average global foreground Dice: [0.8713, 0.9586, 0.9406]\n",
            "2022-01-26 01:07:50.434758: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:07:55.820633: lr: 0.000807\n",
            "2022-01-26 01:07:56.832147: This epoch took 110.197334 s\n",
            "\n",
            "2022-01-26 01:07:58.083355: \n",
            "epoch:  939\n",
            "2022-01-26 01:09:18.266665: train loss : -0.9543\n",
            "2022-01-26 01:09:39.021923: validation loss: -0.8050\n",
            "2022-01-26 01:09:40.228343: Average global foreground Dice: [0.8668, 0.9582, 0.9394]\n",
            "2022-01-26 01:09:42.492974: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:09:47.708311: lr: 0.000795\n",
            "2022-01-26 01:09:48.841770: This epoch took 109.579037 s\n",
            "\n",
            "2022-01-26 01:09:50.878013: \n",
            "epoch:  940\n",
            "2022-01-26 01:11:11.989254: train loss : -0.9528\n",
            "2022-01-26 01:11:32.744457: validation loss: -0.8084\n",
            "2022-01-26 01:11:33.948022: Average global foreground Dice: [0.8673, 0.9584, 0.9404]\n",
            "2022-01-26 01:11:36.376915: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:11:41.196412: lr: 0.000783\n",
            "2022-01-26 01:11:42.394214: This epoch took 109.282198 s\n",
            "\n",
            "2022-01-26 01:11:44.720375: \n",
            "epoch:  941\n",
            "2022-01-26 01:13:06.194446: train loss : -0.9528\n",
            "2022-01-26 01:13:26.679339: validation loss: -0.8132\n",
            "2022-01-26 01:13:27.816148: Average global foreground Dice: [0.867, 0.959, 0.9407]\n",
            "2022-01-26 01:13:29.297382: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:13:34.495337: lr: 0.000771\n",
            "2022-01-26 01:13:35.713329: This epoch took 108.892803 s\n",
            "\n",
            "2022-01-26 01:13:37.993344: \n",
            "epoch:  942\n",
            "2022-01-26 01:14:58.473585: train loss : -0.9529\n",
            "2022-01-26 01:15:19.934577: validation loss: -0.8105\n",
            "2022-01-26 01:15:21.202837: Average global foreground Dice: [0.8695, 0.9593, 0.9417]\n",
            "2022-01-26 01:15:23.604739: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:15:28.611142: lr: 0.000759\n",
            "2022-01-26 01:15:29.805681: This epoch took 109.353233 s\n",
            "\n",
            "2022-01-26 01:15:32.039262: \n",
            "epoch:  943\n",
            "2022-01-26 01:16:53.048893: train loss : -0.9536\n",
            "2022-01-26 01:17:13.727977: validation loss: -0.8114\n",
            "2022-01-26 01:17:15.281264: Average global foreground Dice: [0.8663, 0.9589, 0.9408]\n",
            "2022-01-26 01:17:16.475313: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:17:21.714665: lr: 0.000747\n",
            "2022-01-26 01:17:22.983937: This epoch took 108.744898 s\n",
            "\n",
            "2022-01-26 01:17:25.271352: \n",
            "epoch:  944\n",
            "2022-01-26 01:18:47.329880: train loss : -0.9532\n",
            "2022-01-26 01:19:07.423242: validation loss: -0.8081\n",
            "2022-01-26 01:19:08.736634: Average global foreground Dice: [0.8704, 0.9583, 0.9392]\n",
            "2022-01-26 01:19:10.919640: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:19:15.974504: lr: 0.000735\n",
            "2022-01-26 01:19:17.730581: This epoch took 110.209335 s\n",
            "\n",
            "2022-01-26 01:19:19.943398: \n",
            "epoch:  945\n",
            "2022-01-26 01:20:40.129700: train loss : -0.9536\n",
            "2022-01-26 01:21:01.521767: validation loss: -0.8064\n",
            "2022-01-26 01:21:02.735353: Average global foreground Dice: [0.87, 0.9579, 0.9384]\n",
            "2022-01-26 01:21:05.006098: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:21:10.323704: lr: 0.000723\n",
            "2022-01-26 01:21:11.402058: This epoch took 109.376886 s\n",
            "\n",
            "2022-01-26 01:21:13.747731: \n",
            "epoch:  946\n",
            "2022-01-26 01:22:35.183976: train loss : -0.9534\n",
            "2022-01-26 01:22:55.691195: validation loss: -0.8010\n",
            "2022-01-26 01:22:56.846752: Average global foreground Dice: [0.8618, 0.958, 0.939]\n",
            "2022-01-26 01:22:59.119457: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:23:04.022581: lr: 0.000711\n",
            "2022-01-26 01:23:05.304664: This epoch took 109.535649 s\n",
            "\n",
            "2022-01-26 01:23:07.371692: \n",
            "epoch:  947\n",
            "2022-01-26 01:24:27.884955: train loss : -0.9542\n",
            "2022-01-26 01:24:48.964909: validation loss: -0.8077\n",
            "2022-01-26 01:24:50.099698: Average global foreground Dice: [0.8664, 0.9586, 0.9403]\n",
            "2022-01-26 01:24:52.216182: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:24:57.152038: lr: 0.000699\n",
            "2022-01-26 01:24:58.327190: This epoch took 109.928886 s\n",
            "\n",
            "2022-01-26 01:25:00.484537: \n",
            "epoch:  948\n",
            "2022-01-26 01:26:21.381677: train loss : -0.9538\n",
            "2022-01-26 01:26:42.530504: validation loss: -0.8056\n",
            "2022-01-26 01:26:43.780127: Average global foreground Dice: [0.8708, 0.9584, 0.9399]\n",
            "2022-01-26 01:26:46.070857: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:26:51.213359: lr: 0.000687\n",
            "2022-01-26 01:26:52.533230: This epoch took 109.818511 s\n",
            "\n",
            "2022-01-26 01:26:54.493664: \n",
            "epoch:  949\n",
            "2022-01-26 01:28:15.260933: train loss : -0.9541\n",
            "2022-01-26 01:28:36.067303: validation loss: -0.8115\n",
            "2022-01-26 01:28:37.225350: Average global foreground Dice: [0.8728, 0.9588, 0.9406]\n",
            "2022-01-26 01:28:38.358724: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:28:43.470182: lr: 0.000675\n",
            "2022-01-26 01:28:44.582504: saving scheduled checkpoint file...\n",
            "2022-01-26 01:28:46.771676: saving checkpoint...\n",
            "2022-01-26 01:28:52.664980: done, saving took 5.92 seconds\n",
            "2022-01-26 01:28:55.945077: done\n",
            "2022-01-26 01:28:57.283590: This epoch took 120.544761 s\n",
            "\n",
            "2022-01-26 01:28:59.510040: \n",
            "epoch:  950\n",
            "2022-01-26 01:30:20.363130: train loss : -0.9544\n",
            "2022-01-26 01:30:40.277175: validation loss: -0.8090\n",
            "2022-01-26 01:30:41.588980: Average global foreground Dice: [0.8714, 0.9579, 0.9391]\n",
            "2022-01-26 01:30:42.728761: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:30:47.617838: lr: 0.000662\n",
            "2022-01-26 01:30:48.670285: This epoch took 106.893896 s\n",
            "\n",
            "2022-01-26 01:30:50.882434: \n",
            "epoch:  951\n",
            "2022-01-26 01:32:13.673138: train loss : -0.9535\n",
            "2022-01-26 01:32:33.888274: validation loss: -0.8106\n",
            "2022-01-26 01:32:35.150471: Average global foreground Dice: [0.8739, 0.9583, 0.9393]\n",
            "2022-01-26 01:32:37.122861: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:32:42.194285: lr: 0.00065\n",
            "2022-01-26 01:32:43.374755: This epoch took 110.363822 s\n",
            "\n",
            "2022-01-26 01:32:45.396634: \n",
            "epoch:  952\n",
            "2022-01-26 01:34:07.250860: train loss : -0.9535\n",
            "2022-01-26 01:34:27.393026: validation loss: -0.8079\n",
            "2022-01-26 01:34:28.523564: Average global foreground Dice: [0.8661, 0.9584, 0.9406]\n",
            "2022-01-26 01:34:30.779960: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:34:34.714150: lr: 0.000638\n",
            "2022-01-26 01:34:35.917269: This epoch took 108.316244 s\n",
            "\n",
            "2022-01-26 01:34:38.260504: \n",
            "epoch:  953\n",
            "2022-01-26 01:35:59.858124: train loss : -0.9537\n",
            "2022-01-26 01:36:20.481987: validation loss: -0.8109\n",
            "2022-01-26 01:36:21.789831: Average global foreground Dice: [0.8696, 0.9587, 0.9408]\n",
            "2022-01-26 01:36:23.980134: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:36:29.184278: lr: 0.000626\n",
            "2022-01-26 01:36:30.401883: This epoch took 110.050815 s\n",
            "\n",
            "2022-01-26 01:36:32.769217: \n",
            "epoch:  954\n",
            "2022-01-26 01:37:52.793067: train loss : -0.9544\n",
            "2022-01-26 01:38:13.232394: validation loss: -0.8133\n",
            "2022-01-26 01:38:14.525953: Average global foreground Dice: [0.8695, 0.9585, 0.9396]\n",
            "2022-01-26 01:38:16.624308: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:38:22.345364: lr: 0.000614\n",
            "2022-01-26 01:38:23.456244: This epoch took 109.452716 s\n",
            "\n",
            "2022-01-26 01:38:25.555543: \n",
            "epoch:  955\n",
            "2022-01-26 01:39:45.794832: train loss : -0.9540\n",
            "2022-01-26 01:40:07.024722: validation loss: -0.8110\n",
            "2022-01-26 01:40:08.341521: Average global foreground Dice: [0.8691, 0.9585, 0.9404]\n",
            "2022-01-26 01:40:10.785313: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:40:15.510654: lr: 0.000601\n",
            "2022-01-26 01:40:16.736540: This epoch took 108.884656 s\n",
            "\n",
            "2022-01-26 01:40:18.850339: \n",
            "epoch:  956\n",
            "2022-01-26 01:41:39.460843: train loss : -0.9543\n",
            "2022-01-26 01:41:59.743283: validation loss: -0.8107\n",
            "2022-01-26 01:42:01.135349: Average global foreground Dice: [0.8662, 0.9582, 0.9402]\n",
            "2022-01-26 01:42:03.412856: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:42:08.161367: lr: 0.000589\n",
            "2022-01-26 01:42:09.428227: This epoch took 108.586352 s\n",
            "\n",
            "2022-01-26 01:42:11.706433: \n",
            "epoch:  957\n",
            "2022-01-26 01:43:34.208032: train loss : -0.9543\n",
            "2022-01-26 01:43:53.712926: validation loss: -0.8090\n",
            "2022-01-26 01:43:54.868432: Average global foreground Dice: [0.8673, 0.9584, 0.9397]\n",
            "2022-01-26 01:43:56.927040: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:44:01.797014: lr: 0.000577\n",
            "2022-01-26 01:44:03.019707: This epoch took 109.006998 s\n",
            "\n",
            "2022-01-26 01:44:04.166219: \n",
            "epoch:  958\n",
            "2022-01-26 01:45:25.974429: train loss : -0.9543\n",
            "2022-01-26 01:45:45.958943: validation loss: -0.8039\n",
            "2022-01-26 01:45:47.137740: Average global foreground Dice: [0.8604, 0.9576, 0.9388]\n",
            "2022-01-26 01:45:49.673148: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:45:54.850996: lr: 0.000564\n",
            "2022-01-26 01:45:55.927323: This epoch took 109.373281 s\n",
            "\n",
            "2022-01-26 01:45:57.911854: \n",
            "epoch:  959\n",
            "2022-01-26 01:47:18.902328: train loss : -0.9539\n",
            "2022-01-26 01:47:40.518407: validation loss: -0.8140\n",
            "2022-01-26 01:47:41.698395: Average global foreground Dice: [0.8681, 0.9592, 0.9414]\n",
            "2022-01-26 01:47:43.805411: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:47:48.844076: lr: 0.000552\n",
            "2022-01-26 01:47:50.000664: This epoch took 109.771413 s\n",
            "\n",
            "2022-01-26 01:47:52.245972: \n",
            "epoch:  960\n",
            "2022-01-26 01:49:13.371840: train loss : -0.9537\n",
            "2022-01-26 01:49:34.672328: validation loss: -0.8083\n",
            "2022-01-26 01:49:35.884367: Average global foreground Dice: [0.8694, 0.9585, 0.9404]\n",
            "2022-01-26 01:49:37.853443: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:49:42.823588: lr: 0.000539\n",
            "2022-01-26 01:49:44.199080: This epoch took 109.704342 s\n",
            "\n",
            "2022-01-26 01:49:46.199825: \n",
            "epoch:  961\n",
            "2022-01-26 01:51:07.522064: train loss : -0.9538\n",
            "2022-01-26 01:51:28.585159: validation loss: -0.8107\n",
            "2022-01-26 01:51:29.782441: Average global foreground Dice: [0.8646, 0.9583, 0.9407]\n",
            "2022-01-26 01:51:32.012733: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:51:38.060997: lr: 0.000527\n",
            "2022-01-26 01:51:39.493116: This epoch took 111.069382 s\n",
            "\n",
            "2022-01-26 01:51:41.550516: \n",
            "epoch:  962\n",
            "2022-01-26 01:53:02.489317: train loss : -0.9543\n",
            "2022-01-26 01:53:23.256909: validation loss: -0.8129\n",
            "2022-01-26 01:53:25.104225: Average global foreground Dice: [0.8664, 0.9585, 0.9402]\n",
            "2022-01-26 01:53:27.288351: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:53:31.132663: lr: 0.000514\n",
            "2022-01-26 01:53:32.481391: This epoch took 108.803394 s\n",
            "\n",
            "2022-01-26 01:53:34.517304: \n",
            "epoch:  963\n",
            "2022-01-26 01:54:56.322974: train loss : -0.9543\n",
            "2022-01-26 01:55:16.502267: validation loss: -0.8075\n",
            "2022-01-26 01:55:17.659942: Average global foreground Dice: [0.8672, 0.9578, 0.9385]\n",
            "2022-01-26 01:55:19.907981: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:55:24.751473: lr: 0.000502\n",
            "2022-01-26 01:55:26.409488: This epoch took 109.772157 s\n",
            "\n",
            "2022-01-26 01:55:28.623683: \n",
            "epoch:  964\n",
            "2022-01-26 01:56:50.800443: train loss : -0.9544\n",
            "2022-01-26 01:57:11.439431: validation loss: -0.8054\n",
            "2022-01-26 01:57:12.688958: Average global foreground Dice: [0.8655, 0.9584, 0.94]\n",
            "2022-01-26 01:57:14.778515: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:57:19.942548: lr: 0.000489\n",
            "2022-01-26 01:57:21.039452: This epoch took 110.102705 s\n",
            "\n",
            "2022-01-26 01:57:22.197205: \n",
            "epoch:  965\n",
            "2022-01-26 01:58:43.540532: train loss : -0.9542\n",
            "2022-01-26 01:59:04.346720: validation loss: -0.8102\n",
            "2022-01-26 01:59:05.720947: Average global foreground Dice: [0.8696, 0.9579, 0.9396]\n",
            "2022-01-26 01:59:08.065543: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 01:59:13.188403: lr: 0.000477\n",
            "2022-01-26 01:59:14.329188: This epoch took 110.002966 s\n",
            "\n",
            "2022-01-26 01:59:16.496386: \n",
            "epoch:  966\n",
            "2022-01-26 02:00:37.965988: train loss : -0.9550\n",
            "2022-01-26 02:00:58.957757: validation loss: -0.8077\n",
            "2022-01-26 02:01:00.192862: Average global foreground Dice: [0.8701, 0.9581, 0.9396]\n",
            "2022-01-26 02:01:02.574360: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:01:08.138763: lr: 0.000464\n",
            "2022-01-26 02:01:09.517937: This epoch took 110.869500 s\n",
            "\n",
            "2022-01-26 02:01:11.808944: \n",
            "epoch:  967\n",
            "2022-01-26 02:02:35.053421: train loss : -0.9552\n",
            "2022-01-26 02:02:53.905894: validation loss: -0.8026\n",
            "2022-01-26 02:02:55.474884: Average global foreground Dice: [0.8702, 0.9576, 0.9392]\n",
            "2022-01-26 02:02:57.585872: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:03:02.624743: lr: 0.000451\n",
            "2022-01-26 02:03:03.840044: This epoch took 109.666728 s\n",
            "\n",
            "2022-01-26 02:03:05.874834: \n",
            "epoch:  968\n",
            "2022-01-26 02:04:27.134717: train loss : -0.9544\n",
            "2022-01-26 02:04:48.480018: validation loss: -0.8090\n",
            "2022-01-26 02:04:49.683322: Average global foreground Dice: [0.8669, 0.9584, 0.9403]\n",
            "2022-01-26 02:04:50.851754: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:04:56.073877: lr: 0.000439\n",
            "2022-01-26 02:04:57.221074: This epoch took 109.028706 s\n",
            "\n",
            "2022-01-26 02:04:59.165807: \n",
            "epoch:  969\n",
            "2022-01-26 02:06:20.123203: train loss : -0.9547\n",
            "2022-01-26 02:06:41.032282: validation loss: -0.8089\n",
            "2022-01-26 02:06:42.338619: Average global foreground Dice: [0.8684, 0.9583, 0.9393]\n",
            "2022-01-26 02:06:44.621037: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:06:49.808758: lr: 0.000426\n",
            "2022-01-26 02:06:51.137028: This epoch took 109.744078 s\n",
            "\n",
            "2022-01-26 02:06:53.227802: \n",
            "epoch:  970\n",
            "2022-01-26 02:08:14.396101: train loss : -0.9551\n",
            "2022-01-26 02:08:35.266749: validation loss: -0.8061\n",
            "2022-01-26 02:08:36.391855: Average global foreground Dice: [0.865, 0.9577, 0.9388]\n",
            "2022-01-26 02:08:38.723515: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:08:43.683820: lr: 0.000413\n",
            "2022-01-26 02:08:44.878151: This epoch took 109.335543 s\n",
            "\n",
            "2022-01-26 02:08:47.048534: \n",
            "epoch:  971\n",
            "2022-01-26 02:10:08.806982: train loss : -0.9547\n",
            "2022-01-26 02:10:29.928871: validation loss: -0.8031\n",
            "2022-01-26 02:10:30.977059: Average global foreground Dice: [0.8662, 0.9583, 0.9394]\n",
            "2022-01-26 02:10:33.328129: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:10:38.560734: lr: 0.0004\n",
            "2022-01-26 02:10:39.688243: This epoch took 110.509426 s\n",
            "\n",
            "2022-01-26 02:10:40.864654: \n",
            "epoch:  972\n",
            "2022-01-26 02:12:01.152056: train loss : -0.9545\n",
            "2022-01-26 02:12:21.723046: validation loss: -0.8033\n",
            "2022-01-26 02:12:22.931810: Average global foreground Dice: [0.8637, 0.9579, 0.9387]\n",
            "2022-01-26 02:12:24.347826: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:12:29.539508: lr: 0.000387\n",
            "2022-01-26 02:12:31.157483: This epoch took 108.441620 s\n",
            "\n",
            "2022-01-26 02:12:33.492815: \n",
            "epoch:  973\n",
            "2022-01-26 02:13:54.278487: train loss : -0.9546\n",
            "2022-01-26 02:14:14.808042: validation loss: -0.8062\n",
            "2022-01-26 02:14:16.155874: Average global foreground Dice: [0.8654, 0.9581, 0.9389]\n",
            "2022-01-26 02:14:18.519614: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:14:23.066234: lr: 0.000375\n",
            "2022-01-26 02:14:24.285195: This epoch took 108.545278 s\n",
            "\n",
            "2022-01-26 02:14:26.484670: \n",
            "epoch:  974\n",
            "2022-01-26 02:15:48.283560: train loss : -0.9548\n",
            "2022-01-26 02:16:09.537706: validation loss: -0.8098\n",
            "2022-01-26 02:16:10.768126: Average global foreground Dice: [0.8699, 0.9579, 0.9394]\n",
            "2022-01-26 02:16:13.199985: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:16:18.029361: lr: 0.000362\n",
            "2022-01-26 02:16:19.273050: This epoch took 110.480977 s\n",
            "\n",
            "2022-01-26 02:16:21.606778: \n",
            "epoch:  975\n",
            "2022-01-26 02:17:41.192416: train loss : -0.9550\n",
            "2022-01-26 02:18:01.581452: validation loss: -0.8081\n",
            "2022-01-26 02:18:02.781860: Average global foreground Dice: [0.871, 0.9585, 0.9398]\n",
            "2022-01-26 02:18:04.913531: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:18:10.034609: lr: 0.000348\n",
            "2022-01-26 02:18:11.077287: This epoch took 108.234873 s\n",
            "\n",
            "2022-01-26 02:18:13.046279: \n",
            "epoch:  976\n",
            "2022-01-26 02:19:33.400503: train loss : -0.9545\n",
            "2022-01-26 02:19:54.847416: validation loss: -0.8111\n",
            "2022-01-26 02:19:56.071449: Average global foreground Dice: [0.8703, 0.9583, 0.94]\n",
            "2022-01-26 02:19:58.335292: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:20:04.135124: lr: 0.000335\n",
            "2022-01-26 02:20:05.266009: This epoch took 109.943743 s\n",
            "\n",
            "2022-01-26 02:20:07.514471: \n",
            "epoch:  977\n",
            "2022-01-26 02:21:28.305763: train loss : -0.9549\n",
            "2022-01-26 02:21:49.949184: validation loss: -0.8070\n",
            "2022-01-26 02:21:51.063802: Average global foreground Dice: [0.868, 0.9587, 0.9397]\n",
            "2022-01-26 02:21:53.396330: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:21:58.428551: lr: 0.000322\n",
            "2022-01-26 02:21:59.524847: This epoch took 109.694017 s\n",
            "\n",
            "2022-01-26 02:22:00.702254: \n",
            "epoch:  978\n",
            "2022-01-26 02:23:22.083004: train loss : -0.9549\n",
            "2022-01-26 02:23:42.575165: validation loss: -0.8065\n",
            "2022-01-26 02:23:43.772845: Average global foreground Dice: [0.8669, 0.9583, 0.9398]\n",
            "2022-01-26 02:23:45.812958: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:23:50.725309: lr: 0.000309\n",
            "2022-01-26 02:23:52.073814: This epoch took 109.345081 s\n",
            "\n",
            "2022-01-26 02:23:54.095589: \n",
            "epoch:  979\n",
            "2022-01-26 02:25:14.218991: train loss : -0.9547\n",
            "2022-01-26 02:25:35.321325: validation loss: -0.8067\n",
            "2022-01-26 02:25:36.538035: Average global foreground Dice: [0.8704, 0.9579, 0.9394]\n",
            "2022-01-26 02:25:38.693699: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:25:43.849671: lr: 0.000296\n",
            "2022-01-26 02:25:45.114585: This epoch took 108.787642 s\n",
            "\n",
            "2022-01-26 02:25:47.514815: \n",
            "epoch:  980\n",
            "2022-01-26 02:27:09.725510: train loss : -0.9550\n",
            "2022-01-26 02:27:29.306604: validation loss: -0.8065\n",
            "2022-01-26 02:27:30.477396: Average global foreground Dice: [0.863, 0.9577, 0.9391]\n",
            "2022-01-26 02:27:31.725953: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:27:36.983959: lr: 0.000282\n",
            "2022-01-26 02:27:38.061511: This epoch took 108.247391 s\n",
            "\n",
            "2022-01-26 02:27:40.219501: \n",
            "epoch:  981\n",
            "2022-01-26 02:29:00.958653: train loss : -0.9548\n",
            "2022-01-26 02:29:20.988218: validation loss: -0.8090\n",
            "2022-01-26 02:29:22.255615: Average global foreground Dice: [0.8682, 0.9587, 0.9402]\n",
            "2022-01-26 02:29:24.296455: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:29:29.462717: lr: 0.000269\n",
            "2022-01-26 02:29:30.639298: This epoch took 108.276084 s\n",
            "\n",
            "2022-01-26 02:29:32.835081: \n",
            "epoch:  982\n",
            "2022-01-26 02:30:53.135479: train loss : -0.9550\n",
            "2022-01-26 02:31:14.449286: validation loss: -0.8081\n",
            "2022-01-26 02:31:15.617759: Average global foreground Dice: [0.8714, 0.9581, 0.9402]\n",
            "2022-01-26 02:31:17.991806: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:31:23.035982: lr: 0.000256\n",
            "2022-01-26 02:31:24.286583: This epoch took 109.704902 s\n",
            "\n",
            "2022-01-26 02:31:26.404858: \n",
            "epoch:  983\n",
            "2022-01-26 02:32:49.224167: train loss : -0.9552\n",
            "2022-01-26 02:33:08.113997: validation loss: -0.8084\n",
            "2022-01-26 02:33:09.270829: Average global foreground Dice: [0.8694, 0.9591, 0.9414]\n",
            "2022-01-26 02:33:11.245295: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:33:16.667490: lr: 0.000242\n",
            "2022-01-26 02:33:17.928150: This epoch took 109.440547 s\n",
            "\n",
            "2022-01-26 02:33:20.113278: \n",
            "epoch:  984\n",
            "2022-01-26 02:34:41.210987: train loss : -0.9553\n",
            "2022-01-26 02:35:01.295197: validation loss: -0.8047\n",
            "2022-01-26 02:35:02.548575: Average global foreground Dice: [0.8634, 0.9583, 0.9404]\n",
            "2022-01-26 02:35:04.556843: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:35:09.937934: lr: 0.000228\n",
            "2022-01-26 02:35:11.122636: This epoch took 109.022381 s\n",
            "\n",
            "2022-01-26 02:35:13.125794: \n",
            "epoch:  985\n",
            "2022-01-26 02:36:34.650379: train loss : -0.9548\n",
            "2022-01-26 02:36:55.698120: validation loss: -0.8084\n",
            "2022-01-26 02:36:56.903908: Average global foreground Dice: [0.8709, 0.9585, 0.94]\n",
            "2022-01-26 02:36:59.100078: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:37:04.313083: lr: 0.000215\n",
            "2022-01-26 02:37:05.511778: This epoch took 110.131643 s\n",
            "\n",
            "2022-01-26 02:37:06.755944: \n",
            "epoch:  986\n",
            "2022-01-26 02:38:28.195021: train loss : -0.9551\n",
            "2022-01-26 02:38:49.217603: validation loss: -0.8093\n",
            "2022-01-26 02:38:50.277303: Average global foreground Dice: [0.8707, 0.9584, 0.9393]\n",
            "2022-01-26 02:38:52.408746: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:38:57.476301: lr: 0.000201\n",
            "2022-01-26 02:38:58.649117: This epoch took 109.785949 s\n",
            "\n",
            "2022-01-26 02:38:59.835417: \n",
            "epoch:  987\n",
            "2022-01-26 02:40:21.497406: train loss : -0.9545\n",
            "2022-01-26 02:40:42.383813: validation loss: -0.8072\n",
            "2022-01-26 02:40:43.670676: Average global foreground Dice: [0.869, 0.9583, 0.9406]\n",
            "2022-01-26 02:40:46.035114: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:40:51.025810: lr: 0.000187\n",
            "2022-01-26 02:40:52.158484: This epoch took 110.075840 s\n",
            "\n",
            "2022-01-26 02:40:54.328617: \n",
            "epoch:  988\n",
            "2022-01-26 02:42:14.428715: train loss : -0.9549\n",
            "2022-01-26 02:42:35.244690: validation loss: -0.8079\n",
            "2022-01-26 02:42:36.736651: Average global foreground Dice: [0.8669, 0.9586, 0.9398]\n",
            "2022-01-26 02:42:38.905667: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:42:44.635214: lr: 0.000173\n",
            "2022-01-26 02:42:45.918886: This epoch took 110.365975 s\n",
            "\n",
            "2022-01-26 02:42:48.159529: \n",
            "epoch:  989\n",
            "2022-01-26 02:44:09.519310: train loss : -0.9553\n",
            "2022-01-26 02:44:29.737784: validation loss: -0.8071\n",
            "2022-01-26 02:44:31.023503: Average global foreground Dice: [0.8677, 0.9586, 0.9407]\n",
            "2022-01-26 02:44:33.073699: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:44:38.387952: lr: 0.000158\n",
            "2022-01-26 02:44:39.473024: This epoch took 109.238705 s\n",
            "\n",
            "2022-01-26 02:44:41.806244: \n",
            "epoch:  990\n",
            "2022-01-26 02:46:04.851624: train loss : -0.9552\n",
            "2022-01-26 02:46:23.685313: validation loss: -0.8058\n",
            "2022-01-26 02:46:24.966535: Average global foreground Dice: [0.8679, 0.9582, 0.9403]\n",
            "2022-01-26 02:46:27.251621: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:46:32.688176: lr: 0.000144\n",
            "2022-01-26 02:46:33.893827: This epoch took 109.953307 s\n",
            "\n",
            "2022-01-26 02:46:36.033345: \n",
            "epoch:  991\n",
            "2022-01-26 02:47:58.576396: train loss : -0.9557\n",
            "2022-01-26 02:48:17.603179: validation loss: -0.8078\n",
            "2022-01-26 02:48:18.782330: Average global foreground Dice: [0.869, 0.9583, 0.9402]\n",
            "2022-01-26 02:48:20.037552: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:48:25.011468: lr: 0.00013\n",
            "2022-01-26 02:48:26.168401: This epoch took 108.542288 s\n",
            "\n",
            "2022-01-26 02:48:28.250486: \n",
            "epoch:  992\n",
            "2022-01-26 02:49:49.608486: train loss : -0.9555\n",
            "2022-01-26 02:50:10.454260: validation loss: -0.8101\n",
            "2022-01-26 02:50:11.531207: Average global foreground Dice: [0.871, 0.9585, 0.9401]\n",
            "2022-01-26 02:50:13.922304: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:50:19.038899: lr: 0.000115\n",
            "2022-01-26 02:50:20.249550: This epoch took 109.837605 s\n",
            "\n",
            "2022-01-26 02:50:22.259288: \n",
            "epoch:  993\n",
            "2022-01-26 02:51:42.888186: train loss : -0.9559\n",
            "2022-01-26 02:52:03.798357: validation loss: -0.8056\n",
            "2022-01-26 02:52:04.838263: Average global foreground Dice: [0.8693, 0.958, 0.9387]\n",
            "2022-01-26 02:52:07.192519: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:52:12.270786: lr: 0.0001\n",
            "2022-01-26 02:52:13.407701: This epoch took 109.105507 s\n",
            "\n",
            "2022-01-26 02:52:15.845074: \n",
            "epoch:  994\n",
            "2022-01-26 02:53:36.978037: train loss : -0.9554\n",
            "2022-01-26 02:53:58.024111: validation loss: -0.8061\n",
            "2022-01-26 02:53:59.237606: Average global foreground Dice: [0.8692, 0.9582, 0.9394]\n",
            "2022-01-26 02:54:01.470445: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:54:06.728693: lr: 8.5e-05\n",
            "2022-01-26 02:54:07.887411: This epoch took 110.148216 s\n",
            "\n",
            "2022-01-26 02:54:09.998848: \n",
            "epoch:  995\n",
            "2022-01-26 02:55:31.307759: train loss : -0.9554\n",
            "2022-01-26 02:55:52.197365: validation loss: -0.8088\n",
            "2022-01-26 02:55:53.475133: Average global foreground Dice: [0.8654, 0.9583, 0.9394]\n",
            "2022-01-26 02:55:55.616655: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:56:01.837475: lr: 6.9e-05\n",
            "2022-01-26 02:56:02.971640: This epoch took 110.757695 s\n",
            "\n",
            "2022-01-26 02:56:05.160429: \n",
            "epoch:  996\n",
            "2022-01-26 02:57:27.200552: train loss : -0.9549\n",
            "2022-01-26 02:57:46.903983: validation loss: -0.8069\n",
            "2022-01-26 02:57:48.189913: Average global foreground Dice: [0.8597, 0.9584, 0.9402]\n",
            "2022-01-26 02:57:50.321827: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:57:54.252998: lr: 5.4e-05\n",
            "2022-01-26 02:57:55.395961: This epoch took 107.981242 s\n",
            "\n",
            "2022-01-26 02:57:56.555496: \n",
            "epoch:  997\n",
            "2022-01-26 02:59:18.497797: train loss : -0.9548\n",
            "2022-01-26 02:59:39.500980: validation loss: -0.8098\n",
            "2022-01-26 02:59:40.657556: Average global foreground Dice: [0.8692, 0.9584, 0.9403]\n",
            "2022-01-26 02:59:42.781823: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 02:59:48.080731: lr: 3.7e-05\n",
            "2022-01-26 02:59:49.190803: This epoch took 110.605599 s\n",
            "\n",
            "2022-01-26 02:59:50.231312: \n",
            "epoch:  998\n",
            "2022-01-26 03:01:11.817778: train loss : -0.9553\n",
            "2022-01-26 03:01:31.578046: validation loss: -0.8059\n",
            "2022-01-26 03:01:32.889938: Average global foreground Dice: [0.8636, 0.9579, 0.9394]\n",
            "2022-01-26 03:01:34.894176: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 03:01:39.978279: lr: 2e-05\n",
            "2022-01-26 03:01:41.750613: This epoch took 109.328014 s\n",
            "\n",
            "2022-01-26 03:01:43.984552: \n",
            "epoch:  999\n",
            "2022-01-26 03:03:06.289637: train loss : -0.9553\n",
            "2022-01-26 03:03:25.804290: validation loss: -0.8092\n",
            "2022-01-26 03:03:26.996359: Average global foreground Dice: [0.8708, 0.9585, 0.9405]\n",
            "2022-01-26 03:03:29.389110: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
            "2022-01-26 03:03:34.589024: lr: 0.0\n",
            "2022-01-26 03:03:35.857774: saving scheduled checkpoint file...\n",
            "2022-01-26 03:03:38.003693: saving checkpoint...\n",
            "2022-01-26 03:03:44.211472: done, saving took 6.24 seconds\n",
            "2022-01-26 03:03:46.842716: done\n",
            "2022-01-26 03:03:48.015124: This epoch took 121.733474 s\n",
            "\n",
            "2022-01-26 03:03:50.255023: saving checkpoint...\n",
            "2022-01-26 03:03:57.239139: done, saving took 7.01 seconds\n",
            "IBSR_03 (2, 111, 131, 143)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "force_separate_z: None interpolation order: 1\n",
            "no resampling necessary\n",
            "IBSR_16 (2, 115, 142, 149)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "force_separate_z: None interpolation order: 1\n",
            "separate z: False lowres axis None\n",
            "no separate z, order 1\n",
            "2022-01-26 03:04:35.476417: finished prediction\n",
            "2022-01-26 03:04:36.832562: evaluation of raw predictions\n",
            "2022-01-26 03:04:46.761705: determining postprocessing\n",
            "Foreground vs background\n",
            "before: 0.9260684008937888\n",
            "after:  0.9260674063402411\n",
            "1\n",
            "before: 0.8729678006545709\n",
            "after:  0.8199737377832894\n",
            "2\n",
            "before: 0.9613855361685131\n",
            "after:  0.9615187712710183\n",
            "Removing all but the largest region for class 2 improved results!\n",
            "min_valid_object_sizes None\n",
            "3\n",
            "before: 0.9438518658582825\n",
            "after:  0.891098330036332\n",
            "done\n",
            "for which classes:\n",
            "[2]\n",
            "min_object_sizes\n",
            "None\n",
            "done\n"
          ]
        }
      ],
      "source": [
        "#!nnUNet_train 3d_fullres nnUNetTrainerV2 Task601_IBSR 0\n",
        "!nnUNet_train 2d nnUNetTrainerV2 Task701_IBSR 3 --npz -c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1ya8yhVg8Go"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "function KeepClicking(){\n",
        "   console.log(\"Clicking\");\n",
        "   document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(KeepClicking,60000)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76MELYsc5Fzi"
      },
      "source": [
        "## Identifying the best U-Net configuration \n",
        "\n",
        "* nnUNet_find_best_configuration -m 2d 3d_fullres 3d_lowres 3d_cascade_fullres -t XXX --strict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTLo0zzm7pXw",
        "outputId": "cfe49432-5398-4e0d-fe0e-5d3882063076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Please cite the following paper when using nnUNet:\n",
            "\n",
            "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
            "\n",
            "\n",
            "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
            "\n",
            "running missing postprocessing for Task701_IBSR and model 2d\n",
            "Foreground vs background\n",
            "before: 0.9319618482820086\n",
            "after:  0.9319635201534119\n",
            "1\n",
            "before: 0.9042879089594928\n",
            "after:  0.8550932840909115\n",
            "2\n",
            "before: 0.9496420817800205\n",
            "after:  0.9497000265762005\n",
            "Removing all but the largest region for class 2 improved results!\n",
            "min_valid_object_sizes None\n",
            "3\n",
            "before: 0.9419555541065124\n",
            "after:  0.8952501376778921\n",
            "done\n",
            "for which classes:\n",
            "[2]\n",
            "min_object_sizes\n",
            "None\n",
            "done\n",
            "\n",
            "I will now ensemble combinations of the following models:\n",
            " ['2d']\n",
            "2d 0.9319618482820086\n",
            "Task701_IBSR submit model 2d 0.9319618482820086\n",
            "\n",
            "Here is how you should predict test cases. Run in sequential order and replace all input and output folder names with your personalized ones\n",
            "\n",
            "nnUNet_predict -i FOLDER_WITH_TEST_CASES -o OUTPUT_FOLDER_MODEL1 -tr nnUNetTrainerV2 -ctr nnUNetTrainerV2CascadeFullRes -m 2d -p nnUNetPlansv2.1 -t Task701_IBSR\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!nnUNet_find_best_configuration -m 2d -t 701"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqgoPjec5yLh"
      },
      "source": [
        "## Prediction \n",
        "\n",
        "*  nnUNet_predict -i imagesTs FOLDER -o OUTPUT_DIRECTORY -t TASK_NUMBER -m TRAINER_CLASS_NAME \n",
        "* Sample - nnUNet_predict -i nnUNet_raw_data_base/nnUNet_raw_data/Task601_IBSR/imagesTs/ -o OUTPUT_DIRECTORY -t 601 -m 3d_fullres\n",
        "* For a specific fold: -m 2d -f 0 -t Task701_IBSR --save_npz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NeM_z4q5UsS",
        "outputId": "c2d84ac4-2ee2-4653-a7f8-9eb14ea7d24d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Please cite the following paper when using nnUNet:\n",
            "\n",
            "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
            "\n",
            "\n",
            "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
            "\n",
            "using model stored in  /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/results/nnUNet/2d/Task701_IBSR/nnUNetTrainerV2__nnUNetPlansv2.1\n",
            "This model expects 1 input modalities for each image\n",
            "Found 3 unique case ids, here are some examples: ['IBSR_10' 'IBSR_15' 'IBSR_15']\n",
            "If they don't look right, make sure to double check your filenames. They must end with _0000.nii.gz etc\n",
            "number of cases: 3\n",
            "number of cases that still need to be predicted: 3\n",
            "emptying cuda cache\n",
            "loading parameters for folds, None\n",
            "folds is None so we will automatically look for output folders (not using 'all'!)\n",
            "found the following folds:  ['/content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/results/nnUNet/2d/Task701_IBSR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0', '/content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/results/nnUNet/2d/Task701_IBSR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1', '/content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/results/nnUNet/2d/Task701_IBSR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_2', '/content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/results/nnUNet/2d/Task701_IBSR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_3', '/content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/results/nnUNet/2d/Task701_IBSR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_4']\n",
            "using the following model files:  ['/content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/results/nnUNet/2d/Task701_IBSR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model', '/content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/results/nnUNet/2d/Task701_IBSR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/model_final_checkpoint.model', '/content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/results/nnUNet/2d/Task701_IBSR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_2/model_final_checkpoint.model', '/content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/results/nnUNet/2d/Task701_IBSR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_3/model_final_checkpoint.model', '/content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/results/nnUNet/2d/Task701_IBSR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_4/model_final_checkpoint.model']\n",
            "starting preprocessing generator\n",
            "starting prediction...\n",
            "preprocessing /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/ibsr_original_test_images_predicted/IBSR_02.nii.gz\n",
            "using preprocessor PreprocessorFor2D\n",
            "preprocessing /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/ibsr_original_test_images_predicted/IBSR_10.nii.gz\n",
            "using preprocessor PreprocessorFor2D\n",
            "preprocessing /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/ibsr_original_test_images_predicted/IBSR_15.nii.gz\n",
            "using preprocessor PreprocessorFor2D\n",
            "before crop: (1, 256, 128, 256) after crop: (1, 147, 120, 147) spacing: [0.9375 1.5    0.9375] \n",
            "\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([0.9375, 1.5   , 0.9375]), 'spacing_transposed': array([1.5   , 0.9375, 0.9375]), 'data.shape (data is transposed)': (1, 120, 147, 147)} \n",
            "after:  {'spacing': array([1.5   , 0.9375, 0.9375]), 'data.shape (data is resampled)': (1, 120, 147, 147)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "(1, 120, 147, 147)\n",
            "This worker has ended successfully, no errors to report\n",
            "predicting /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/ibsr_original_test_images_predicted/IBSR_02.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "before crop: (1, 256, 128, 256) after crop: (1, 134, 119, 137) spacing: [1.  1.5 1. ] \n",
            "\n",
            "no separate z, order 3\n",
            "before crop: (1, 256, 128, 256) after crop: (1, 156, 119, 160) spacing: [0.8370536 1.5       0.8370536] \n",
            "\n",
            "no separate z, order 3\n",
            "no separate z, order 1\n",
            "no separate z, order 1\n",
            "before: {'spacing': array([1. , 1.5, 1. ]), 'spacing_transposed': array([1.5, 1. , 1. ]), 'data.shape (data is transposed)': (1, 119, 134, 137)} \n",
            "after:  {'spacing': array([1.5   , 0.9375, 0.9375]), 'data.shape (data is resampled)': (1, 119, 143, 146)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "(1, 119, 143, 146)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "before: {'spacing': array([0.8370536, 1.5      , 0.8370536]), 'spacing_transposed': array([1.5      , 0.8370536, 0.8370536]), 'data.shape (data is transposed)': (1, 119, 156, 160)} \n",
            "after:  {'spacing': array([1.5   , 0.9375, 0.9375]), 'data.shape (data is resampled)': (1, 119, 139, 143)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "(1, 119, 139, 143)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "predicting /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/ibsr_original_test_images_predicted/IBSR_10.nii.gz\n",
            "force_separate_z: None interpolation order: 1\n",
            "no resampling necessary\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "This worker has ended successfully, no errors to report\n",
            "predicting /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/ibsr_original_test_images_predicted/IBSR_15.nii.gz\n",
            "force_separate_z: None interpolation order: 1\n",
            "separate z: False lowres axis None\n",
            "no separate z, order 1\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "This worker has ended successfully, no errors to report\n",
            "force_separate_z: None interpolation order: 1\n",
            "separate z: False lowres axis None\n",
            "no separate z, order 1\n",
            "inference done. Now waiting for the segmentation export to finish...\n",
            "postprocessing...\n"
          ]
        }
      ],
      "source": [
        "#FOLDER_WITH_TEST_CASES = /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/2d_prediction_601-multi\n",
        "#OUTPUT_FOLDER_MODEL1 = /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/nnUNet_raw_data_base/nnUNet_raw_data/Task601_IBSR/imagesTs/\n",
        "\n",
        "!nnUNet_predict -i /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/nnUNet_raw_data_base/nnUNet_raw_data/Task701_IBSR/Original_Test_Set/ -o /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/ibsr_original_test_images_predicted -tr nnUNetTrainerV2 -ctr nnUNetTrainerV2CascadeFullRes -m 2d -p nnUNetPlansv2.1 -t Task701_IBSR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTUV7_qlqnil"
      },
      "source": [
        "## Train MultiClass\n",
        "\n",
        "If you ran initially without the --npz flag but now require the softmax predictions, simply run\n",
        "\n",
        "* nnUNet_train CONFIGURATION TRAINER_CLASS_NAME TASK_NAME_OR_ID FOLD -val --npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mMh98Bi8Gz0",
        "outputId": "667f4792-a7a1-48da-cd2d-0aea7d675c5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Please cite the following paper when using nnUNet:\n",
            "\n",
            "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
            "\n",
            "\n",
            "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
            "\n",
            "###############################################\n",
            "I am running the following nnUNet: 2d\n",
            "My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>\n",
            "For that I will be using the following configuration:\n",
            "num_classes:  3\n",
            "modalities:  {0: 'T1-w'}\n",
            "use_mask_for_norm OrderedDict([(0, True)])\n",
            "keep_only_largest_region None\n",
            "min_region_size_per_class None\n",
            "min_size_per_class None\n",
            "normalization_schemes OrderedDict([(0, 'nonCT')])\n",
            "stages...\n",
            "\n",
            "stage:  0\n",
            "{'batch_size': 45, 'num_pool_per_axis': [5, 5], 'patch_size': array([160, 160]), 'median_patient_size_in_voxels': array([115, 139, 144]), 'current_spacing': array([1.5   , 0.9375, 0.9375]), 'original_spacing': array([1.5   , 0.9375, 0.9375]), 'pool_op_kernel_sizes': [[2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'do_dummy_2D_data_aug': False}\n",
            "\n",
            "I am using stage 0 from these plans\n",
            "I am using batch dice + CE loss\n",
            "\n",
            "I am using data from this folder:  /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/nnUNet_preprocessed/Task601_IBSR/nnUNetData_plans_v2.1_2D\n",
            "###############################################\n",
            "2022-01-16 11:01:55.045020: loading checkpoint /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/results/nnUNet/2d/Task601_IBSR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model train= False\n",
            "loading dataset\n",
            "loading all case properties\n",
            "2022-01-16 11:02:00.163562: Using splits from existing split file: /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/nnUNet_preprocessed/Task601_IBSR/splits_final.pkl\n",
            "2022-01-16 11:02:00.453488: The split file contains 5 splits.\n",
            "2022-01-16 11:02:00.455956: Desired fold for training: 0\n",
            "2022-01-16 11:02:00.458774: This split has 8 training and 2 validation cases.\n",
            "IBSR_01 (2, 117, 139, 145)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "force_separate_z: None interpolation order: 1\n",
            "no resampling necessary\n",
            "IBSR_09 (2, 115, 147, 147)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "force_separate_z: None interpolation order: 1\n",
            "separate z: False lowres axis None\n",
            "no separate z, order 1\n",
            "2022-01-16 11:02:11.642167: finished prediction\n",
            "2022-01-16 11:02:11.644438: evaluation of raw predictions\n",
            "/usr/local/lib/python3.7/dist-packages/nnunet/evaluation/evaluator.py:381: RuntimeWarning: Mean of empty slice\n",
            "  all_scores[\"mean\"][label][score] = float(np.nanmean(all_scores[\"mean\"][label][score]))\n",
            "2022-01-16 11:02:13.228302: determining postprocessing\n",
            "Foreground vs background\n",
            "before: 0.0006397947220419517\n",
            "after:  0.0006397988705209942\n",
            "Removing all but the largest foreground region improved results!\n",
            "for_which_classes [1, 2, 3]\n",
            "min_valid_object_sizes None\n",
            "1\n",
            "before: 0.0019193966115629824\n",
            "after:  0.0019193966115629824\n",
            "2\n",
            "before: 0.0\n",
            "after:  0.0\n",
            "3\n",
            "before: 0.0\n",
            "after:  0.0\n",
            "done\n",
            "for which classes:\n",
            "[[1, 2, 3]]\n",
            "min_object_sizes\n",
            "None\n",
            "done\n",
            "\n",
            "\n",
            "Please cite the following paper when using nnUNet:\n",
            "\n",
            "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
            "\n",
            "\n",
            "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
            "\n",
            "###############################################\n",
            "I am running the following nnUNet: 2d\n",
            "My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>\n",
            "For that I will be using the following configuration:\n",
            "num_classes:  3\n",
            "modalities:  {0: 'T1-w'}\n",
            "use_mask_for_norm OrderedDict([(0, True)])\n",
            "keep_only_largest_region None\n",
            "min_region_size_per_class None\n",
            "min_size_per_class None\n",
            "normalization_schemes OrderedDict([(0, 'nonCT')])\n",
            "stages...\n",
            "\n",
            "stage:  0\n",
            "{'batch_size': 45, 'num_pool_per_axis': [5, 5], 'patch_size': array([160, 160]), 'median_patient_size_in_voxels': array([115, 139, 144]), 'current_spacing': array([1.5   , 0.9375, 0.9375]), 'original_spacing': array([1.5   , 0.9375, 0.9375]), 'pool_op_kernel_sizes': [[2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'do_dummy_2D_data_aug': False}\n",
            "\n",
            "I am using stage 0 from these plans\n",
            "I am using batch dice + CE loss\n",
            "\n",
            "I am using data from this folder:  /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/nnUNet_preprocessed/Task601_IBSR/nnUNetData_plans_v2.1_2D\n",
            "###############################################\n",
            "2022-01-16 11:02:30.295301: loading checkpoint /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/results/nnUNet/2d/Task601_IBSR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/model_final_checkpoint.model train= False\n",
            "loading dataset\n",
            "loading all case properties\n",
            "2022-01-16 11:02:32.652892: Using splits from existing split file: /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/nnUNet_preprocessed/Task601_IBSR/splits_final.pkl\n",
            "2022-01-16 11:02:32.655772: The split file contains 5 splits.\n",
            "2022-01-16 11:02:32.657477: Desired fold for training: 1\n",
            "2022-01-16 11:02:32.659262: This split has 8 training and 2 validation cases.\n",
            "IBSR_05 (2, 115, 137, 140)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "force_separate_z: None interpolation order: 1\n",
            "no resampling necessary\n",
            "IBSR_18 (2, 121, 154, 148)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "force_separate_z: None interpolation order: 1\n",
            "separate z: False lowres axis None\n",
            "no separate z, order 1\n",
            "2022-01-16 11:02:45.155345: finished prediction\n",
            "2022-01-16 11:02:45.157736: evaluation of raw predictions\n",
            "/usr/local/lib/python3.7/dist-packages/nnunet/evaluation/evaluator.py:381: RuntimeWarning: Mean of empty slice\n",
            "  all_scores[\"mean\"][label][score] = float(np.nanmean(all_scores[\"mean\"][label][score]))\n",
            "2022-01-16 11:02:46.560906: determining postprocessing\n",
            "Foreground vs background\n",
            "before: 0.0010615751790490002\n",
            "after:  0.0010615850585860701\n",
            "Removing all but the largest foreground region improved results!\n",
            "for_which_classes [1, 2, 3]\n",
            "min_valid_object_sizes None\n",
            "1\n",
            "before: 0.0031847551757582106\n",
            "after:  0.0031847551757582106\n",
            "2\n",
            "before: 0.0\n",
            "after:  0.0\n",
            "3\n",
            "before: 0.0\n",
            "after:  0.0\n",
            "done\n",
            "for which classes:\n",
            "[[1, 2, 3]]\n",
            "min_object_sizes\n",
            "None\n",
            "done\n",
            "\n",
            "\n",
            "Please cite the following paper when using nnUNet:\n",
            "\n",
            "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
            "\n",
            "\n",
            "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
            "\n",
            "###############################################\n",
            "I am running the following nnUNet: 2d\n",
            "My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>\n",
            "For that I will be using the following configuration:\n",
            "num_classes:  3\n",
            "modalities:  {0: 'T1-w'}\n",
            "use_mask_for_norm OrderedDict([(0, True)])\n",
            "keep_only_largest_region None\n",
            "min_region_size_per_class None\n",
            "min_size_per_class None\n",
            "normalization_schemes OrderedDict([(0, 'nonCT')])\n",
            "stages...\n",
            "\n",
            "stage:  0\n",
            "{'batch_size': 45, 'num_pool_per_axis': [5, 5], 'patch_size': array([160, 160]), 'median_patient_size_in_voxels': array([115, 139, 144]), 'current_spacing': array([1.5   , 0.9375, 0.9375]), 'original_spacing': array([1.5   , 0.9375, 0.9375]), 'pool_op_kernel_sizes': [[2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'do_dummy_2D_data_aug': False}\n",
            "\n",
            "I am using stage 0 from these plans\n",
            "I am using batch dice + CE loss\n",
            "\n",
            "I am using data from this folder:  /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/nnUNet_preprocessed/Task601_IBSR/nnUNetData_plans_v2.1_2D\n",
            "###############################################\n",
            "2022-01-16 11:02:58.314718: loading checkpoint /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/results/nnUNet/2d/Task601_IBSR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_2/model_final_checkpoint.model train= False\n",
            "loading dataset\n",
            "loading all case properties\n",
            "2022-01-16 11:03:01.959992: Using splits from existing split file: /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/nnUNet_preprocessed/Task601_IBSR/splits_final.pkl\n",
            "2022-01-16 11:03:01.962889: The split file contains 5 splits.\n",
            "2022-01-16 11:03:01.964941: Desired fold for training: 2\n",
            "2022-01-16 11:03:01.966868: This split has 8 training and 2 validation cases.\n",
            "IBSR_06 (2, 119, 142, 136)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "force_separate_z: None interpolation order: 1\n",
            "no resampling necessary\n",
            "IBSR_08 (2, 109, 139, 144)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "force_separate_z: None interpolation order: 1\n",
            "separate z: False lowres axis None\n",
            "no separate z, order 1\n",
            "2022-01-16 11:03:13.098974: finished prediction\n",
            "2022-01-16 11:03:13.101275: evaluation of raw predictions\n",
            "/usr/local/lib/python3.7/dist-packages/nnunet/evaluation/evaluator.py:381: RuntimeWarning: Mean of empty slice\n",
            "  all_scores[\"mean\"][label][score] = float(np.nanmean(all_scores[\"mean\"][label][score]))\n",
            "2022-01-16 11:03:14.582853: determining postprocessing\n",
            "Foreground vs background\n",
            "before: 0.0014398471253743545\n",
            "after:  0.0014398503688875987\n",
            "Removing all but the largest foreground region improved results!\n",
            "for_which_classes [1, 2, 3]\n",
            "min_valid_object_sizes None\n",
            "1\n",
            "before: 0.004319551106662796\n",
            "after:  0.004319551106662796\n",
            "2\n",
            "before: 0.0\n",
            "after:  0.0\n",
            "3\n",
            "before: 0.0\n",
            "after:  0.0\n",
            "done\n",
            "for which classes:\n",
            "[[1, 2, 3]]\n",
            "min_object_sizes\n",
            "None\n",
            "done\n",
            "\n",
            "\n",
            "Please cite the following paper when using nnUNet:\n",
            "\n",
            "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
            "\n",
            "\n",
            "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
            "\n",
            "###############################################\n",
            "I am running the following nnUNet: 2d\n",
            "My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>\n",
            "For that I will be using the following configuration:\n",
            "num_classes:  3\n",
            "modalities:  {0: 'T1-w'}\n",
            "use_mask_for_norm OrderedDict([(0, True)])\n",
            "keep_only_largest_region None\n",
            "min_region_size_per_class None\n",
            "min_size_per_class None\n",
            "normalization_schemes OrderedDict([(0, 'nonCT')])\n",
            "stages...\n",
            "\n",
            "stage:  0\n",
            "{'batch_size': 45, 'num_pool_per_axis': [5, 5], 'patch_size': array([160, 160]), 'median_patient_size_in_voxels': array([115, 139, 144]), 'current_spacing': array([1.5   , 0.9375, 0.9375]), 'original_spacing': array([1.5   , 0.9375, 0.9375]), 'pool_op_kernel_sizes': [[2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'do_dummy_2D_data_aug': False}\n",
            "\n",
            "I am using stage 0 from these plans\n",
            "I am using batch dice + CE loss\n",
            "\n",
            "I am using data from this folder:  /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/nnUNet_preprocessed/Task601_IBSR/nnUNetData_plans_v2.1_2D\n",
            "###############################################\n",
            "2022-01-16 11:03:26.154360: loading checkpoint /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/results/nnUNet/2d/Task601_IBSR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_3/model_final_checkpoint.model train= False\n",
            "loading dataset\n",
            "loading all case properties\n",
            "2022-01-16 11:03:28.998177: Using splits from existing split file: /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/nnUNet_preprocessed/Task601_IBSR/splits_final.pkl\n",
            "2022-01-16 11:03:29.020465: The split file contains 5 splits.\n",
            "2022-01-16 11:03:29.022460: Desired fold for training: 3\n",
            "2022-01-16 11:03:29.024759: This split has 8 training and 2 validation cases.\n",
            "IBSR_03 (2, 111, 131, 143)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "force_separate_z: None interpolation order: 1\n",
            "no resampling necessary\n",
            "IBSR_16 (2, 115, 142, 149)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "force_separate_z: None interpolation order: 1\n",
            "separate z: False lowres axis None\n",
            "no separate z, order 1\n",
            "2022-01-16 11:03:40.432447: finished prediction\n",
            "2022-01-16 11:03:40.434590: evaluation of raw predictions\n",
            "/usr/local/lib/python3.7/dist-packages/nnunet/evaluation/evaluator.py:381: RuntimeWarning: Mean of empty slice\n",
            "  all_scores[\"mean\"][label][score] = float(np.nanmean(all_scores[\"mean\"][label][score]))\n",
            "2022-01-16 11:03:41.734171: determining postprocessing\n",
            "Foreground vs background\n",
            "before: 0.0008880107149581978\n",
            "after:  0.0008880161395046544\n",
            "Removing all but the largest foreground region improved results!\n",
            "for_which_classes [1, 2, 3]\n",
            "min_valid_object_sizes None\n",
            "1\n",
            "before: 0.002664048418513963\n",
            "after:  0.002664048418513963\n",
            "2\n",
            "before: 0.0\n",
            "after:  0.0\n",
            "3\n",
            "before: 0.0\n",
            "after:  0.0\n",
            "done\n",
            "for which classes:\n",
            "[[1, 2, 3]]\n",
            "min_object_sizes\n",
            "None\n",
            "done\n",
            "\n",
            "\n",
            "Please cite the following paper when using nnUNet:\n",
            "\n",
            "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
            "\n",
            "\n",
            "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
            "\n",
            "###############################################\n",
            "I am running the following nnUNet: 2d\n",
            "My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>\n",
            "For that I will be using the following configuration:\n",
            "num_classes:  3\n",
            "modalities:  {0: 'T1-w'}\n",
            "use_mask_for_norm OrderedDict([(0, True)])\n",
            "keep_only_largest_region None\n",
            "min_region_size_per_class None\n",
            "min_size_per_class None\n",
            "normalization_schemes OrderedDict([(0, 'nonCT')])\n",
            "stages...\n",
            "\n",
            "stage:  0\n",
            "{'batch_size': 45, 'num_pool_per_axis': [5, 5], 'patch_size': array([160, 160]), 'median_patient_size_in_voxels': array([115, 139, 144]), 'current_spacing': array([1.5   , 0.9375, 0.9375]), 'original_spacing': array([1.5   , 0.9375, 0.9375]), 'pool_op_kernel_sizes': [[2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'do_dummy_2D_data_aug': False}\n",
            "\n",
            "I am using stage 0 from these plans\n",
            "I am using batch dice + CE loss\n",
            "\n",
            "I am using data from this folder:  /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/nnUNet_preprocessed/Task601_IBSR/nnUNetData_plans_v2.1_2D\n",
            "###############################################\n",
            "2022-01-16 11:03:53.117874: loading checkpoint /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/results/nnUNet/2d/Task601_IBSR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_4/model_final_checkpoint.model train= False\n",
            "loading dataset\n",
            "loading all case properties\n",
            "2022-01-16 11:03:56.183848: Using splits from existing split file: /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/nnUNet_preprocessed/Task601_IBSR/splits_final.pkl\n",
            "2022-01-16 11:03:56.186495: The split file contains 5 splits.\n",
            "2022-01-16 11:03:56.188329: Desired fold for training: 4\n",
            "2022-01-16 11:03:56.190273: This split has 8 training and 2 validation cases.\n",
            "IBSR_04 (2, 116, 134, 151)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "force_separate_z: None interpolation order: 1\n",
            "no resampling necessary\n",
            "IBSR_07 (2, 113, 137, 141)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "force_separate_z: None interpolation order: 1\n",
            "separate z: False lowres axis None\n",
            "no separate z, order 1\n",
            "2022-01-16 11:04:07.063065: finished prediction\n",
            "2022-01-16 11:04:07.065329: evaluation of raw predictions\n",
            "/usr/local/lib/python3.7/dist-packages/nnunet/evaluation/evaluator.py:381: RuntimeWarning: Mean of empty slice\n",
            "  all_scores[\"mean\"][label][score] = float(np.nanmean(all_scores[\"mean\"][label][score]))\n",
            "2022-01-16 11:04:08.382849: determining postprocessing\n",
            "Foreground vs background\n",
            "before: 0.0007140921744158177\n",
            "after:  0.0007140958998624686\n",
            "Removing all but the largest foreground region improved results!\n",
            "for_which_classes [1, 2, 3]\n",
            "min_valid_object_sizes None\n",
            "1\n",
            "before: 0.002142287699587406\n",
            "after:  0.002142287699587406\n",
            "2\n",
            "before: 0.0\n",
            "after:  0.0\n",
            "3\n",
            "before: 0.0\n",
            "after:  0.0\n",
            "done\n",
            "for which classes:\n",
            "[[1, 2, 3]]\n",
            "min_object_sizes\n",
            "None\n",
            "done\n"
          ]
        }
      ],
      "source": [
        "\"\"\"#!nnUNet_train -h\n",
        "\n",
        "!nnUNet_train 2d nnUNetTrainerV2 Task601_IBSR 0 -val --npz\n",
        "!nnUNet_train 2d nnUNetTrainerV2 Task601_IBSR 1 -val --npz\n",
        "!nnUNet_train 2d nnUNetTrainerV2 Task601_IBSR 2 -val --npz\n",
        "!nnUNet_train 2d nnUNetTrainerV2 Task601_IBSR 3 -val --npz\n",
        "!nnUNet_train 2d nnUNetTrainerV2 Task601_IBSR 4 -val --npz\n",
        "\n",
        "\n",
        "!nnUNet_find_best_configuration -m 2d -t 601\n",
        "!nnUNet_predict -i /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/nnUNet_raw_data_base/nnUNet_raw_data/Task601_IBSR/imagesTs/ -o /content/drive/MyDrive/MISA/NN_Unet/NNUnet_with_IBSR/2d_prediction_601-multi -tr nnUNetTrainerV2 -ctr nnUNetTrainerV2CascadeFullRes -m 2d -p nnUNetPlansv2.1 -t Task601_IBSR --save_npz\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "sjmKuaXtpDBt",
        "X-mF1QkPpJ3T",
        "JdhyHUZLpTJu",
        "fE-MMz54pYLm",
        "76MELYsc5Fzi",
        "XTUV7_qlqnil"
      ],
      "machine_shape": "hm",
      "name": "Adilina - nnUNet with IBSR .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}