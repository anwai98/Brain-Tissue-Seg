{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMIG5gOwa-Nb"
      },
      "source": [
        "##Importing the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spA2vbr3a0j7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ee2219-23c2-4304-fca4-06c911888cf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: simpleitk in /usr/local/lib/python3.7/dist-packages (2.1.1)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import glob\n",
        "import warnings\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "!pip install simpleitk\n",
        "import SimpleITK as sitk\n",
        "from scipy import ndimage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, Dropout, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, Input\n",
        "\n",
        "from google.colab import drive\n",
        "drive._mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEFUu8FjYvaz"
      },
      "source": [
        "##Defining the Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eerKWEtxUg_x"
      },
      "outputs": [],
      "source": [
        "# Image Parameters\n",
        "IMAGE_SIZE = (256, 128, 256)\n",
        "\n",
        "# Training, Testing and Validation Parameters\n",
        "TRAINING_VOLUMES = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
        "VALIDATION_VOLUMES = [9]\n",
        "\n",
        "# Hyperparameters\n",
        "N_CLASSES = 4\n",
        "N_INPUT_CHANNELS = 1\n",
        "PATCH_SIZE = (32, 32)\n",
        "PATCH_STRIDE = (32, 32)\n",
        "\n",
        "# Data Preparation Parameters\n",
        "CONTENT_THRESHOLD = 0.3 # To Get Rid of Useless Information in the Image\n",
        "\n",
        "# Training Parameters\n",
        "N_EPOCHS = 200\n",
        "BATCH_SIZE = 64\n",
        "PATIENCE = 20\n",
        "MODEL_FNAME_PATTERN = 'model.h5'\n",
        "OPTIMISER = 'Adam'\n",
        "LOSS = 'categorical_crossentropy'\n",
        "dropout_rate = 0.40"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kakQO5X9E0oR"
      },
      "source": [
        "##Define UNet Architecture"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unet(img_size=PATCH_SIZE, n_classes=N_CLASSES, n_input_channels=N_INPUT_CHANNELS, scale=1):\n",
        "    inputs = keras.Input(shape=img_size + (n_input_channels, ))\n",
        "\n",
        "    # Encoding Path of the DenseUNet (32-64-128-256-512)\n",
        "    conv11 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conc11 = concatenate([inputs, conv11], axis=3)\n",
        "    conv12 = Conv2D(32, (3, 3), activation='relu', padding='same')(conc11)\n",
        "    conc12 = concatenate([inputs, conv12], axis=3)\n",
        "    drop1 = Dropout(rate=dropout_rate)(conc12, training=True)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(drop1)\n",
        "\n",
        "    conv21 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    conc21 = concatenate([pool1, conv21], axis=3)\n",
        "    conv22 = Conv2D(64, (3, 3), activation='relu', padding='same')(conc21)\n",
        "    conc22 = concatenate([pool1, conv22], axis=3)\n",
        "    drop2 = Dropout(rate=dropout_rate)(conc22, training=True)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(drop2)\n",
        "\n",
        "    conv31 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "    conc31 = concatenate([pool2, conv31], axis=3)\n",
        "    conv32 = Conv2D(128, (3, 3), activation='relu', padding='same')(conc31)\n",
        "    conc32 = concatenate([pool2, conv32], axis=3)\n",
        "    drop3 = Dropout(rate=dropout_rate)(conc32, training=True)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(drop3)\n",
        "\n",
        "    conv41 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
        "    conc41 = concatenate([pool3, conv41], axis=3)\n",
        "    conv42 = Conv2D(256, (3, 3), activation='relu', padding='same')(conc41)\n",
        "    conc42 = concatenate([pool3, conv42], axis=3)\n",
        "    drop4 = Dropout(rate=dropout_rate)(conc42, training=True)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    conv51 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
        "    conc51 = concatenate([pool4, conv51], axis=3)\n",
        "    conv52 = Conv2D(512, (3, 3), activation='relu', padding='same')(conc51)\n",
        "    conc52 = concatenate([pool4, conv52], axis=3)\n",
        "    drop5 = Dropout(rate=dropout_rate)(conc52, training=True)\n",
        "\n",
        "    # Decoding Path of the ResUNet\n",
        "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(drop5), conc42], axis=3)\n",
        "    conv61 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
        "    conc61 = concatenate([up6, conv61], axis=3)\n",
        "    conv62 = Conv2D(256, (3, 3), activation='relu', padding='same')(conc61)\n",
        "    conc62 = concatenate([up6, conv62], axis=3)\n",
        "    drop6 = Dropout(rate=dropout_rate)(conc62, training=True)\n",
        "\n",
        "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(drop6), conv32], axis=3)\n",
        "    conv71 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
        "    conc71 = concatenate([up7, conv71], axis=3)\n",
        "    conv72 = Conv2D(128, (3, 3), activation='relu', padding='same')(conc71)\n",
        "    conc72 = concatenate([up7, conv72], axis=3)\n",
        "    drop7 = Dropout(rate=dropout_rate)(conc72, training=True)\n",
        "\n",
        "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(drop7), conv22], axis=3)\n",
        "    conv81 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
        "    conc81 = concatenate([up8, conv81], axis=3)\n",
        "    conv82 = Conv2D(64, (3, 3), activation='relu', padding='same')(conc81)\n",
        "    conc82 = concatenate([up8, conv82], axis=3)\n",
        "    drop8 = Dropout(rate=dropout_rate)(conc82, training=True)\n",
        "\n",
        "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(drop8), conv12], axis=3)\n",
        "    conv91 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
        "    conc91 = concatenate([up9, conv91], axis=3)\n",
        "    conv92 = Conv2D(32, (3, 3), activation='relu', padding='same')(conc91)\n",
        "    conc92 = concatenate([up9, conv92], axis=3)\n",
        "    drop9 = Dropout(rate=dropout_rate)(conc92, training=True)\n",
        "\n",
        "    outputs = Conv2D(n_classes, (1, 1), activation='sigmoid')(drop9)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "wcAmfYyQoM_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z1E1IglE-0w"
      },
      "source": [
        "##Loading the training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgujGm9tX_Rj"
      },
      "outputs": [],
      "source": [
        "def load_data_bias(image_size, setName) :\n",
        "  \n",
        "  data_file = '/content/drive/My Drive/MISA/Normal Segmentations/data/{}/*'.format(setName)\n",
        "\n",
        "  folders = glob.glob(data_file)\n",
        "  n_volumes = len(folders)\n",
        "  \n",
        "  volumes = np.zeros((n_volumes, *image_size, 1))\n",
        "  labels = np.zeros((n_volumes, *image_size, 1))\n",
        "\n",
        "  i = 0\n",
        "\n",
        "  for filename in glob.glob(data_file):\n",
        "    \n",
        "    #print(filename)\n",
        "    name = filename[-7:]\n",
        "    #print(name)\n",
        "\n",
        "    img_data = nib.load('/content/drive/My Drive/MISA/Normal Segmentations/data/{}/{}/{}_bias_corrected.nii.gz'.format(setName, name, name))\n",
        "    img_data_temp = img_data.get_fdata()\n",
        "    img_data_temp = img_data_temp.reshape((*image_size, 1))\n",
        "    #print(img_data_temp.shape)\n",
        "    volumes[i] = img_data_temp\n",
        "\n",
        "    seg_data = nib.load('/content/drive/My Drive/MISA/Normal Segmentations/data/{}/{}/{}_seg.nii.gz'.format(setName, name, name))\n",
        "    labels[i] = seg_data.get_fdata()\n",
        "    \n",
        "    print(\"Working on image {0}\".format(name))\n",
        "    i = i+1\n",
        "\n",
        "  return (volumes, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoSMUWMWgKQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "594ec935-fb14-40b7-f985-5783f87166fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on image IBSR_05\n",
            "Working on image IBSR_03\n",
            "Working on image IBSR_08\n",
            "Working on image IBSR_04\n",
            "Working on image IBSR_01\n",
            "Working on image IBSR_16\n",
            "Working on image IBSR_18\n",
            "Working on image IBSR_07\n",
            "Working on image IBSR_09\n",
            "Working on image IBSR_06\n",
            "Working on image IBSR_14\n",
            "Working on image IBSR_17\n",
            "Working on image IBSR_12\n",
            "Working on image IBSR_13\n"
          ]
        }
      ],
      "source": [
        "(t_volumes, t_labels) = load_data_bias(IMAGE_SIZE, 'Training_Set')\n",
        "(v_volumes, v_labels) = load_data_bias(IMAGE_SIZE, 'Validation_Set')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAok6QlCeBm0"
      },
      "source": [
        "##Splitting the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_aGLgc60ceM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3da5bbf-028f-42f9-979a-f33ec8ffe90e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 256, 128, 256, 1)\n",
            "(1, 256, 128, 256, 1)\n"
          ]
        }
      ],
      "source": [
        "# Split the training data into training and validation\n",
        "training_volumes = t_volumes[TRAINING_VOLUMES]\n",
        "training_labels = t_labels[TRAINING_VOLUMES]\n",
        "\n",
        "validation_volumes = t_volumes[VALIDATION_VOLUMES]\n",
        "validation_labels = t_labels[VALIDATION_VOLUMES]\n",
        "\n",
        "print(training_volumes.shape)\n",
        "#print(training_labels.shape)\n",
        "\n",
        "print(validation_volumes.shape)\n",
        "#print(validation_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpKWGJ-Fh1B_"
      },
      "source": [
        "##Extracting Patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMvPbAZ4gb8C"
      },
      "outputs": [],
      "source": [
        "def extract_patches(x, patch_size, patch_stride) :\n",
        "  return tf.image.extract_patches(\n",
        "    x,\n",
        "    sizes=[1, *patch_size, 1],\n",
        "    strides=[1, *patch_stride, 1],\n",
        "    rates=[1, 1, 1, 1],\n",
        "    padding='SAME', name=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsKNTm-Lf-sb"
      },
      "outputs": [],
      "source": [
        "def extract_useful_patches(\n",
        "    volumes, labels,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    stride=PATCH_STRIDE,\n",
        "    threshold=CONTENT_THRESHOLD,\n",
        "    num_classes=N_CLASSES) :\n",
        "\n",
        "  volumes = volumes.reshape([-1, image_size[1], image_size[2], 1])\n",
        "  labels = labels.reshape([-1, image_size[1], image_size[2], 1])\n",
        "\n",
        "  vol_patches = extract_patches(volumes, patch_size, stride).numpy()\n",
        "  seg_patches = extract_patches(labels, patch_size, stride).numpy()\n",
        "\n",
        "  vol_patches = vol_patches.reshape([-1, *patch_size, 1])\n",
        "  seg_patches = seg_patches.reshape([-1, *patch_size, ])\n",
        "\n",
        "  # this will get rid of the background and only take foreground\n",
        "  foreground_mask = seg_patches != 0\n",
        "\n",
        "  # we only keep the useful forground patches\n",
        "  # threshold too small - takes even the useless patches\n",
        "  # threshold too high - might leave out useful patches\n",
        "  useful_patches = foreground_mask.sum(axis=(1, 2)) > threshold * np.prod(patch_size)\n",
        "\n",
        "  vol_patches = vol_patches[useful_patches]\n",
        "  seg_patches = seg_patches[useful_patches]\n",
        "\n",
        "  seg_patches = tf.keras.utils.to_categorical(\n",
        "    seg_patches, num_classes=N_CLASSES, dtype='float32')\n",
        "  \n",
        "  return (vol_patches, seg_patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTcoEi426bfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5066f01-02bb-49ef-b948-77049c39b6c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11546, 32, 32, 1)\n"
          ]
        }
      ],
      "source": [
        "# extract patches from training set\n",
        "(training_patches, training_patches_seg) = extract_useful_patches(training_volumes, training_labels)\n",
        "\n",
        "# extract patches from validation set\n",
        "(validation_patches, validation_patches_seg) = extract_useful_patches(validation_volumes, validation_labels)\n",
        "\n",
        "print(training_patches.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx9fsloJgxrm"
      },
      "source": [
        "##Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Degree of Augmentation\n",
        "deg     = 0.2\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=40, #40\n",
        "        width_shift_range=deg,\n",
        "        height_shift_range=deg,\n",
        "        # rescale=1./255,\n",
        "        shear_range=deg,\n",
        "        zoom_range=deg,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "        fill_mode='nearest') #reflect, wrap, constant(black)"
      ],
      "metadata": {
        "id": "CwyB93VgyuIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6o21TRkXedUO"
      },
      "outputs": [],
      "source": [
        "train_generator = datagen.flow(training_patches, batch_size=int(training_patches.shape[0]/BATCH_SIZE), seed=1)\n",
        "train_label_generator = datagen.flow(training_patches_seg, batch_size=int(training_patches.shape[0]/BATCH_SIZE), seed=1)\n",
        "\n",
        "val_generator = datagen.flow(validation_patches, batch_size=int(validation_patches.shape[0]/BATCH_SIZE), seed=1)\n",
        "val_label_generator = datagen.flow(validation_patches_seg, batch_size=int(validation_patches.shape[0]/BATCH_SIZE), seed=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOw8_p4Oedag"
      },
      "outputs": [],
      "source": [
        "X_train = train_generator.next()\n",
        "y_train = train_label_generator.next()\n",
        "\n",
        "X_val = val_generator.next()\n",
        "y_val = val_label_generator.next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IELRGzM3r2mz",
        "outputId": "6c7598af-2f7d-4350-f353-a922af46fbc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11546, 32, 32, 1)\n",
            "(11546, 32, 32, 4)\n",
            "----------------\n",
            "(1201, 32, 32, 1)\n",
            "(1201, 32, 32, 4)\n"
          ]
        }
      ],
      "source": [
        "print(training_patches.shape)\n",
        "print(training_patches_seg.shape)\n",
        "print(\"----------------\")\n",
        "print(validation_patches.shape)\n",
        "print(validation_patches_seg.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce8mcx6Nr3QG",
        "outputId": "7b9c3fa4-b175-4af2-bda4-d5d21535e30f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11726, 32, 32, 1)\n",
            "(11726, 32, 32, 4)\n",
            "(1219, 32, 32, 1)\n",
            "(1219, 32, 32, 4)\n"
          ]
        }
      ],
      "source": [
        "full_train = np.concatenate((training_patches, X_train))\n",
        "print(full_train.shape)\n",
        "full_train_label = np.concatenate((training_patches_seg, y_train))\n",
        "print(full_train_label.shape)\n",
        "\n",
        "full_val = np.concatenate((validation_patches, X_val))\n",
        "print(full_val.shape)\n",
        "full_val_label = np.concatenate((validation_patches_seg, y_val))\n",
        "print(full_val_label.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fdqvKEK1UES"
      },
      "source": [
        "##Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7H9hIPwEwJZ",
        "outputId": "d562ecd7-443d-459a-aa6b-3cc6618a7638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "184/184 [==============================] - 27s 135ms/step - loss: 1.0389 - val_loss: 0.5890\n",
            "Epoch 2/200\n",
            "184/184 [==============================] - 24s 131ms/step - loss: 0.4410 - val_loss: 0.4919\n",
            "Epoch 3/200\n",
            "184/184 [==============================] - 24s 131ms/step - loss: 0.3838 - val_loss: 0.4056\n",
            "Epoch 4/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.3393 - val_loss: 0.4225\n",
            "Epoch 5/200\n",
            "184/184 [==============================] - 24s 132ms/step - loss: 0.3046 - val_loss: 0.3648\n",
            "Epoch 6/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.2799 - val_loss: 0.3749\n",
            "Epoch 7/200\n",
            "184/184 [==============================] - 24s 130ms/step - loss: 0.2612 - val_loss: 0.3762\n",
            "Epoch 8/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.2467 - val_loss: 0.4251\n",
            "Epoch 9/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.2360 - val_loss: 0.3782\n",
            "Epoch 10/200\n",
            "184/184 [==============================] - 24s 129ms/step - loss: 0.2214 - val_loss: 0.3845\n",
            "Epoch 11/200\n",
            "184/184 [==============================] - 24s 132ms/step - loss: 0.2187 - val_loss: 0.3620\n",
            "Epoch 12/200\n",
            "184/184 [==============================] - 24s 132ms/step - loss: 0.2087 - val_loss: 0.3613\n",
            "Epoch 13/200\n",
            "184/184 [==============================] - 24s 131ms/step - loss: 0.1960 - val_loss: 0.3802\n",
            "Epoch 14/200\n",
            "184/184 [==============================] - 24s 129ms/step - loss: 0.1924 - val_loss: 0.3636\n",
            "Epoch 15/200\n",
            "184/184 [==============================] - 24s 129ms/step - loss: 0.1884 - val_loss: 0.3824\n",
            "Epoch 16/200\n",
            "184/184 [==============================] - 25s 134ms/step - loss: 0.1869 - val_loss: 0.3255\n",
            "Epoch 17/200\n",
            "184/184 [==============================] - 24s 129ms/step - loss: 0.1801 - val_loss: 0.3864\n",
            "Epoch 18/200\n",
            "184/184 [==============================] - 24s 132ms/step - loss: 0.1781 - val_loss: 0.3225\n",
            "Epoch 19/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.1796 - val_loss: 0.3412\n",
            "Epoch 20/200\n",
            "184/184 [==============================] - 24s 129ms/step - loss: 0.1725 - val_loss: 0.3855\n",
            "Epoch 21/200\n",
            "184/184 [==============================] - 24s 130ms/step - loss: 0.1675 - val_loss: 0.3887\n",
            "Epoch 22/200\n",
            "184/184 [==============================] - 24s 129ms/step - loss: 0.1648 - val_loss: 0.3573\n",
            "Epoch 23/200\n",
            "184/184 [==============================] - 24s 131ms/step - loss: 0.1609 - val_loss: 0.3656\n",
            "Epoch 24/200\n",
            "184/184 [==============================] - 24s 129ms/step - loss: 0.1594 - val_loss: 0.3815\n",
            "Epoch 25/200\n",
            "184/184 [==============================] - 24s 131ms/step - loss: 0.1584 - val_loss: 0.3340\n",
            "Epoch 26/200\n",
            "184/184 [==============================] - 24s 129ms/step - loss: 0.1568 - val_loss: 0.3988\n",
            "Epoch 27/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.1534 - val_loss: 0.3609\n",
            "Epoch 28/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.1546 - val_loss: 0.3632\n",
            "Epoch 29/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.1475 - val_loss: 0.3988\n",
            "Epoch 30/200\n",
            "184/184 [==============================] - 24s 130ms/step - loss: 0.1514 - val_loss: 0.3571\n",
            "Epoch 31/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.1454 - val_loss: 0.3367\n",
            "Epoch 32/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.1434 - val_loss: 0.3545\n",
            "Epoch 33/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.1431 - val_loss: 0.3638\n",
            "Epoch 34/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.1425 - val_loss: 0.3828\n",
            "Epoch 35/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.1421 - val_loss: 0.3889\n",
            "Epoch 36/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.1408 - val_loss: 0.3678\n",
            "Epoch 37/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.1376 - val_loss: 0.3982\n",
            "Epoch 38/200\n",
            "184/184 [==============================] - 24s 131ms/step - loss: 0.1389 - val_loss: 0.3169\n",
            "Epoch 39/200\n",
            "184/184 [==============================] - 24s 129ms/step - loss: 0.1383 - val_loss: 0.3638\n",
            "Epoch 40/200\n",
            "184/184 [==============================] - 24s 130ms/step - loss: 0.1365 - val_loss: 0.3427\n",
            "Epoch 41/200\n",
            "184/184 [==============================] - 24s 129ms/step - loss: 0.1394 - val_loss: 0.3857\n",
            "Epoch 42/200\n",
            "184/184 [==============================] - 24s 130ms/step - loss: 0.1368 - val_loss: 0.4051\n",
            "Epoch 43/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.1344 - val_loss: 0.4068\n",
            "Epoch 44/200\n",
            "184/184 [==============================] - 24s 130ms/step - loss: 0.1308 - val_loss: 0.3572\n",
            "Epoch 45/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.1328 - val_loss: 0.3591\n",
            "Epoch 46/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.1322 - val_loss: 0.4179\n",
            "Epoch 47/200\n",
            "184/184 [==============================] - 24s 130ms/step - loss: 0.1292 - val_loss: 0.3687\n",
            "Epoch 48/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.1289 - val_loss: 0.3685\n",
            "Epoch 49/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.1294 - val_loss: 0.3894\n",
            "Epoch 50/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.1285 - val_loss: 0.3718\n",
            "Epoch 51/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.1274 - val_loss: 0.3991\n",
            "Epoch 52/200\n",
            "184/184 [==============================] - 24s 129ms/step - loss: 0.1270 - val_loss: 0.4431\n",
            "Epoch 53/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.1260 - val_loss: 0.3765\n",
            "Epoch 54/200\n",
            "184/184 [==============================] - 24s 130ms/step - loss: 0.1253 - val_loss: 0.3385\n",
            "Epoch 55/200\n",
            "184/184 [==============================] - 24s 129ms/step - loss: 0.1280 - val_loss: 0.3674\n",
            "Epoch 56/200\n",
            "184/184 [==============================] - 24s 128ms/step - loss: 0.1298 - val_loss: 0.3446\n",
            "Epoch 57/200\n",
            "184/184 [==============================] - 24s 129ms/step - loss: 0.1252 - val_loss: 0.3855\n",
            "Epoch 58/200\n",
            "184/184 [==============================] - 24s 129ms/step - loss: 0.1220 - val_loss: 0.3703\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7feb8e8724d0>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "my_callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=PATIENCE), # early stopping\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath=MODEL_FNAME_PATTERN, save_best_only=True) # save the best based on validation\n",
        "]\n",
        "\n",
        "unet = get_unet()\n",
        "unet.compile(optimizer=OPTIMISER, loss=LOSS)\n",
        "unet.fit(\n",
        "    x=full_train, \n",
        "    y=full_train_label,\n",
        "    validation_data=(full_val, full_val_label),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=N_EPOCHS,\n",
        "    callbacks=my_callbacks,\n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGcfrZCMjz4M"
      },
      "source": [
        "##Load the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fNsYJL7KnON"
      },
      "outputs": [],
      "source": [
        "unet = get_unet(\n",
        "    img_size=(IMAGE_SIZE[1], IMAGE_SIZE[2]),\n",
        "    n_classes=N_CLASSES,\n",
        "    n_input_channels=N_INPUT_CHANNELS)\n",
        "unet.compile(optimizer=OPTIMISER, loss=LOSS)\n",
        "unet.load_weights('model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwPzN0jrSm8r"
      },
      "source": [
        "##Prepare test data using the validation volumes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuRHLNYGkDFp"
      },
      "outputs": [],
      "source": [
        "def prepare_val_data(the_volumes, the_labels):\n",
        "  testing_volumes_processed = the_volumes.reshape([-1, IMAGE_SIZE[1], IMAGE_SIZE[2], 1])\n",
        "  testing_labels_processed = the_labels.reshape([-1, IMAGE_SIZE[1], IMAGE_SIZE[2], 1])\n",
        "\n",
        "  testing_labels_processed = tf.keras.utils.to_categorical(testing_labels_processed, num_classes=4, dtype='float32')\n",
        "\n",
        "  #print(testing_volumes_processed.shape)\n",
        "  #print(testing_labels_processed.shape)\n",
        "\n",
        "  return (testing_volumes_processed, testing_labels_processed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbJZUQyLqF4p"
      },
      "source": [
        "###Predict labels for test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItoY31x0K3r8"
      },
      "outputs": [],
      "source": [
        "def pred_val_data(testing_volumes_processed)  :\n",
        "  # creates probability map of each label for all volumes\n",
        "  prediction = unet.predict(x=testing_volumes_processed)\n",
        "\n",
        "  prediction = np.argmax(prediction, axis=3)\n",
        "\n",
        "  #plt.axis('off')\n",
        "  #plt.imshow(prediction[:, :, 150])\n",
        "\n",
        "  return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VU2XAj_wxgJv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4cb0a67-a4f0-4c49-e901-1fa6d75e6554"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nprint(prediction.shape)\\nprint(testing_labels_processed.shape)\\nprint(testing_volumes_T1_processed.shape)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "\"\"\"\n",
        "print(prediction.shape)\n",
        "print(testing_labels_processed.shape)\n",
        "print(testing_volumes_T1_processed.shape)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Computing Dice, AVD and HD (Final)\n",
        "\n"
      ],
      "metadata": {
        "id": "5RHaF_LhpWXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_hausdorff_distance(in1, in2, label = 'all'):\n",
        "    in1=in1.squeeze()\n",
        "    in2=in2.squeeze()\n",
        "    hausdorff_distance_filter = sitk.HausdorffDistanceImageFilter()\n",
        "    if label == 'all':\n",
        "        # Hausdorff distance\n",
        "        hausdorff_distance_filter.Execute(in1, in2)\n",
        "    else:\n",
        "        in1_array  = in1 #sitk.GetArrayFromImage(in1)\n",
        "        in1_array = (in1_array == label) *1 \n",
        "        in1_array = in1_array.astype('uint16')  \n",
        "        img1 = sitk.GetImageFromArray(in1_array)\n",
        "        \n",
        "        in2_array  = in2 #sitk.GetArrayFromImage(in2)\n",
        "        in2_array = (in2_array == label) *1 \n",
        "        in2_array = in2_array.astype('uint16')  \n",
        "        img2 = sitk.GetImageFromArray(in2_array)\n",
        "        # Hausdorff distance\n",
        "        hausdorff_distance_filter.Execute(img1, img2)\n",
        "    return hausdorff_distance_filter.GetHausdorffDistance()\n",
        "\n",
        "def compute_dice_coefficient(in1, in2, label  = 'all'):\n",
        "    in1=in1.squeeze()\n",
        "    in2=in2.squeeze()\n",
        "    if label=='all': \n",
        "        return 2 * np.sum( (in1>0) &  (in2>0) & (in1 == in2)) / (np.sum(in1 > 0) + np.sum(in2 > 0))\n",
        "    else:\n",
        "        return 2 * np.sum((in1 == label) & (in2 == label)) / (np.sum(in1 == label) + np.sum(in2 == label))\n",
        "\n",
        "def compute_volumentric_difference(in1, in2, label  = 'all'):\n",
        "    in1=in1.squeeze()\n",
        "    in2=in2.squeeze()\n",
        "    if label  == 'all':\n",
        "      #  vol_dif  = np.sum((in1 != in2) & (in1 !=0) & (in2 !=0))\n",
        "        return np.sum((in1 != in2)) / ((np.sum(in1 > 0) + np.sum(in2 > 0)))\n",
        "    else:\n",
        "        in1  = (in1 == label) * 1\n",
        "        in2  = (in2 == label) * 1\n",
        "        return np.sum((in1 != in2)) / ((np.sum(in1 > 0) + np.sum(in2 > 0)))"
      ],
      "metadata": {
        "id": "iFBnx17Bpa_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cl in range(0,4,1): \n",
        "  overallDSC = np.zeros(N_CLASSES)\n",
        "  overall_Hausdorff = np.zeros(N_CLASSES)\n",
        "  overall_vol = np.zeros(N_CLASSES)\n",
        "\n",
        "  for i in range(0,validation_volumes.shape[0], 1):\n",
        "      \n",
        "      testing_volumes_processed, testing_labels_processed = prepare_val_data(v_volumes[i], v_labels[i])\n",
        "      prediction = pred_val_data(testing_volumes_processed)\n",
        "      \n",
        "      #cl = 3\n",
        "\n",
        "      cur_DSC = compute_dice_coefficient(prediction, v_labels[i], label=cl)\n",
        "      overallDSC = overallDSC + cur_DSC\n",
        "\n",
        "      cur_Hausdorff = compute_hausdorff_distance(prediction, v_labels[i], label=cl) \n",
        "      overall_Hausdorff = overall_Hausdorff + cur_Hausdorff\n",
        "\n",
        "      cur_vol = compute_volumentric_difference(prediction, v_labels[i], label=cl)\n",
        "      overall_vol = overall_vol + cur_vol\n",
        "      \n",
        "      #print(prediction.shape)\n",
        "      #print(v_labels[i].shape)\n",
        "      \n",
        "  #print(overall_Hausdorff)\n",
        "  overallDSC = overallDSC/validation_volumes.shape[0]\n",
        "  overall_Hausdorff = overall_Hausdorff/validation_volumes.shape[0]\n",
        "  overall_vol = overall_vol/validation_volumes.shape[0]\n",
        "\n",
        "  # for i in range(0,cl,1):\n",
        "  #print(\"Class {} - Dice Coefficient = {:.4f}\".format(cl, overallDSC[i]))\n",
        "  #print(\"Class {} - HD = {:.4f}\".format(cl, overall_Hausdorff[i]))\n",
        "  #print(\"Class {} - AVD = {:.4f}\".format(cl, overall_vol[i]))\n",
        "  print(\"Class {}\".format(cl))\n",
        "  print(\"\\tDice Coefficient = {:.4f}\".format(overallDSC[i]))\n",
        "  # print(\"\\tHD = {:.4f}\".format(overall_Hausdorff[i]))\n",
        "  # print(\"\\tAVD = {:.4f}\".format(overall_vol[i]))"
      ],
      "metadata": {
        "id": "8PrStjpepejb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b555035a-37a3-4d60-aeb9-98c4b7e0d4ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0\n",
            "\tDice Coefficient = 0.9913\n",
            "Class 1\n",
            "\tDice Coefficient = 0.7088\n",
            "Class 2\n",
            "\tDice Coefficient = 0.7517\n",
            "Class 3\n",
            "\tDice Coefficient = 0.4736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3zTHEeiN4kE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e09bb416-de1f-4751-aa5d-ac158820e8c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nClass 0 - Dice Coefficient 0.9975\\nClass 1 - Dice Coefficient 0.8342\\nClass 2 - Dice Coefficient 0.9209\\nClass 3 - Dice Coefficient 0.8825\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# 2DUNet\n",
        "# batch size = 32, patient = 5, dropout=0.15, epoch = 50\n",
        "\"\"\"\n",
        "Class 0 - Dice Coefficient 0.9976\n",
        "Class 1 - Dice Coefficient 0.8288\n",
        "Class 2 - Dice Coefficient 0.9186\n",
        "Class 3 - Dice Coefficient 0.8765\n",
        "\"\"\"\n",
        "\n",
        "# batch size = 40, patient = 5, dropout=0.15, epoch = 50\n",
        "\"\"\"\n",
        "Class 0 - Dice Coefficient 0.9977\n",
        "Class 1 - Dice Coefficient 0.8261\n",
        "Class 2 - Dice Coefficient 0.9202\n",
        "Class 3 - Dice Coefficient 0.8790\n",
        "\"\"\"\n",
        "\n",
        "# batch size = 50, patient = 20, dropout=0.15, epoch = 200\n",
        "\"\"\"\n",
        "Class 0 - Dice Coefficient 0.9977\n",
        "Class 1 - Dice Coefficient 0.8261\n",
        "Class 2 - Dice Coefficient 0.9202\n",
        "Class 3 - Dice Coefficient 0.8790\n",
        "\"\"\"\n",
        "\n",
        "# batch size = 64, patient = 20, dropout=0.40, epoch = 200\n",
        "\"\"\"\n",
        "Class 0 - Dice Coefficient 0.9975\n",
        "Class 1 - Dice Coefficient 0.8342\n",
        "Class 2 - Dice Coefficient 0.9209\n",
        "Class 3 - Dice Coefficient 0.8825\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CipR5ZtUqSDC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ByBoNkPGWKrN",
        "uZ2BevUuIkGs"
      ],
      "name": "2DDenseUNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}