{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMIG5gOwa-Nb"
      },
      "source": [
        "##Importing the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spA2vbr3a0j7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eb35b37-3c18-4a19-ead7-9c1d4dd6c944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: simpleitk in /usr/local/lib/python3.7/dist-packages (2.1.1)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import glob\n",
        "import warnings\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "!pip install simpleitk\n",
        "import SimpleITK as sitk\n",
        "from scipy import ndimage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, Dropout, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, Input\n",
        "\n",
        "from google.colab import drive\n",
        "drive._mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEFUu8FjYvaz"
      },
      "source": [
        "##Defining the Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eerKWEtxUg_x"
      },
      "outputs": [],
      "source": [
        "# Image Parameters\n",
        "IMAGE_SIZE = (256, 128, 256)\n",
        "\n",
        "# Training, Testing and Validation Parameters\n",
        "TRAINING_VOLUMES = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
        "VALIDATION_VOLUMES = [9]\n",
        "\n",
        "# Hyperparameters\n",
        "N_CLASSES = 4\n",
        "N_INPUT_CHANNELS = 1\n",
        "PATCH_SIZE = (32, 32)\n",
        "PATCH_STRIDE = (32, 32)\n",
        "\n",
        "# Data Preparation Parameters\n",
        "CONTENT_THRESHOLD = 0.3 # To Get Rid of Useless Information in the Image\n",
        "\n",
        "# Training Parameters\n",
        "N_EPOCHS = 200\n",
        "BATCH_SIZE = 64\n",
        "PATIENCE = 20\n",
        "MODEL_FNAME_PATTERN = 'model.h5'\n",
        "OPTIMISER = 'Adam'\n",
        "LOSS = 'categorical_crossentropy'\n",
        "dropout_rate = 0.40"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kakQO5X9E0oR"
      },
      "source": [
        "##Define UNet Architecture"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unet(img_size=PATCH_SIZE, n_classes=N_CLASSES, n_input_channels=N_INPUT_CHANNELS, scale=1):\n",
        "    inputs = keras.Input(shape=img_size + (n_input_channels, ))\n",
        "\n",
        "    # Encoding Path of the ResUNet (32-64-128-256-512)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    conc1 = concatenate([inputs, conv1], axis=3)\n",
        "    drop1 = Dropout(rate=dropout_rate)(conc1, training=True)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(drop1)\n",
        "\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    conc2 = concatenate([pool1, conv2], axis=3)\n",
        "    drop2 = Dropout(rate=dropout_rate)(conc2, training=True)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(drop2)\n",
        "\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    conc3 = concatenate([pool2, conv3], axis=3)\n",
        "    drop3 = Dropout(rate=dropout_rate)(conc3, training=True)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(drop3)\n",
        "\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    conc4 = concatenate([pool3, conv4], axis=3)\n",
        "    drop4 = Dropout(rate=dropout_rate)(conc4, training=True)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
        "    conc5 = concatenate([pool4, conv5], axis=3)\n",
        "    drop5 = Dropout(rate=dropout_rate)(conc5, training=True)\n",
        "\n",
        "    # Decoding Path of the ResUNet\n",
        "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(drop5), conv4], axis=3)\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
        "    conc6 = concatenate([up6, conv6], axis=3)\n",
        "    drop6 = Dropout(rate=dropout_rate)(conc6, training=True)\n",
        "\n",
        "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(drop6), conv3], axis=3)\n",
        "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
        "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
        "    conc7 = concatenate([up7, conv7], axis=3)\n",
        "    drop7 = Dropout(rate=dropout_rate)(conc7, training=True)\n",
        "\n",
        "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(drop7), conv2], axis=3)\n",
        "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
        "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
        "    conc8 = concatenate([up8, conv8], axis=3)\n",
        "    drop8 = Dropout(rate=dropout_rate)(conc8, training=True)\n",
        "\n",
        "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(drop8), conv1], axis=3)\n",
        "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
        "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
        "    conc9 = concatenate([up9, conv9], axis=3)\n",
        "    drop9 = Dropout(rate=dropout_rate)(conc9, training=True)\n",
        "\n",
        "    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(drop9)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "wcAmfYyQoM_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z1E1IglE-0w"
      },
      "source": [
        "##Loading the training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgujGm9tX_Rj"
      },
      "outputs": [],
      "source": [
        "def load_data_bias(image_size, setName) :\n",
        "  \n",
        "  data_file = '/content/drive/My Drive/MISA/Normal Segmentations/data/{}/*'.format(setName)\n",
        "\n",
        "  folders = glob.glob(data_file)\n",
        "  n_volumes = len(folders)\n",
        "  \n",
        "  volumes = np.zeros((n_volumes, *image_size, 1))\n",
        "  labels = np.zeros((n_volumes, *image_size, 1))\n",
        "\n",
        "  i = 0\n",
        "\n",
        "  for filename in glob.glob(data_file):\n",
        "    \n",
        "    #print(filename)\n",
        "    name = filename[-7:]\n",
        "    #print(name)\n",
        "\n",
        "    img_data = nib.load('/content/drive/My Drive/MISA/Normal Segmentations/data/{}/{}/{}_bias_corrected.nii.gz'.format(setName, name, name))\n",
        "    img_data_temp = img_data.get_fdata()\n",
        "    img_data_temp = img_data_temp.reshape((*image_size, 1))\n",
        "    #print(img_data_temp.shape)\n",
        "    volumes[i] = img_data_temp\n",
        "\n",
        "    seg_data = nib.load('/content/drive/My Drive/MISA/Normal Segmentations/data/{}/{}/{}_seg.nii.gz'.format(setName, name, name))\n",
        "    labels[i] = seg_data.get_fdata()\n",
        "    \n",
        "    print(\"Working on image {0}\".format(name))\n",
        "    i = i+1\n",
        "\n",
        "  return (volumes, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoSMUWMWgKQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3e3f70b-7d7b-436a-aa04-a3a1c6b330a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on image IBSR_05\n",
            "Working on image IBSR_03\n",
            "Working on image IBSR_08\n",
            "Working on image IBSR_04\n",
            "Working on image IBSR_01\n",
            "Working on image IBSR_16\n",
            "Working on image IBSR_18\n",
            "Working on image IBSR_07\n",
            "Working on image IBSR_09\n",
            "Working on image IBSR_06\n",
            "Working on image IBSR_14\n",
            "Working on image IBSR_17\n",
            "Working on image IBSR_12\n",
            "Working on image IBSR_13\n"
          ]
        }
      ],
      "source": [
        "(t_volumes, t_labels) = load_data_bias(IMAGE_SIZE, 'Training_Set')\n",
        "(v_volumes, v_labels) = load_data_bias(IMAGE_SIZE, 'Validation_Set')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAok6QlCeBm0"
      },
      "source": [
        "##Splitting the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_aGLgc60ceM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef43d003-8bde-4bc2-e533-777bbd89e6ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 256, 128, 256, 1)\n",
            "(1, 256, 128, 256, 1)\n"
          ]
        }
      ],
      "source": [
        "# Split the training data into training and validation\n",
        "training_volumes = t_volumes[TRAINING_VOLUMES]\n",
        "training_labels = t_labels[TRAINING_VOLUMES]\n",
        "\n",
        "validation_volumes = t_volumes[VALIDATION_VOLUMES]\n",
        "validation_labels = t_labels[VALIDATION_VOLUMES]\n",
        "\n",
        "print(training_volumes.shape)\n",
        "#print(training_labels.shape)\n",
        "\n",
        "print(validation_volumes.shape)\n",
        "#print(validation_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpKWGJ-Fh1B_"
      },
      "source": [
        "##Extracting Patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMvPbAZ4gb8C"
      },
      "outputs": [],
      "source": [
        "def extract_patches(x, patch_size, patch_stride) :\n",
        "  return tf.image.extract_patches(\n",
        "    x,\n",
        "    sizes=[1, *patch_size, 1],\n",
        "    strides=[1, *patch_stride, 1],\n",
        "    rates=[1, 1, 1, 1],\n",
        "    padding='SAME', name=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsKNTm-Lf-sb"
      },
      "outputs": [],
      "source": [
        "def extract_useful_patches(\n",
        "    volumes, labels,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    stride=PATCH_STRIDE,\n",
        "    threshold=CONTENT_THRESHOLD,\n",
        "    num_classes=N_CLASSES) :\n",
        "\n",
        "  volumes = volumes.reshape([-1, image_size[1], image_size[2], 1])\n",
        "  labels = labels.reshape([-1, image_size[1], image_size[2], 1])\n",
        "\n",
        "  vol_patches = extract_patches(volumes, patch_size, stride).numpy()\n",
        "  seg_patches = extract_patches(labels, patch_size, stride).numpy()\n",
        "\n",
        "  vol_patches = vol_patches.reshape([-1, *patch_size, 1])\n",
        "  seg_patches = seg_patches.reshape([-1, *patch_size, ])\n",
        "\n",
        "  # this will get rid of the background and only take foreground\n",
        "  foreground_mask = seg_patches != 0\n",
        "\n",
        "  # we only keep the useful forground patches\n",
        "  # threshold too small - takes even the useless patches\n",
        "  # threshold too high - might leave out useful patches\n",
        "  useful_patches = foreground_mask.sum(axis=(1, 2)) > threshold * np.prod(patch_size)\n",
        "\n",
        "  vol_patches = vol_patches[useful_patches]\n",
        "  seg_patches = seg_patches[useful_patches]\n",
        "\n",
        "  seg_patches = tf.keras.utils.to_categorical(\n",
        "    seg_patches, num_classes=N_CLASSES, dtype='float32')\n",
        "  \n",
        "  return (vol_patches, seg_patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTcoEi426bfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df974215-2805-41a4-8bd4-aec0047f5200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11546, 32, 32, 1)\n"
          ]
        }
      ],
      "source": [
        "# extract patches from training set\n",
        "(training_patches, training_patches_seg) = extract_useful_patches(training_volumes, training_labels)\n",
        "\n",
        "# extract patches from validation set\n",
        "(validation_patches, validation_patches_seg) = extract_useful_patches(validation_volumes, validation_labels)\n",
        "\n",
        "print(training_patches.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx9fsloJgxrm"
      },
      "source": [
        "##Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Degree of Augmentation\n",
        "deg     = 0.2\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=40, #40\n",
        "        width_shift_range=deg,\n",
        "        height_shift_range=deg,\n",
        "        # rescale=1./255,\n",
        "        shear_range=deg,\n",
        "        zoom_range=deg,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "        fill_mode='nearest') #reflect, wrap, constant(black)"
      ],
      "metadata": {
        "id": "CwyB93VgyuIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6o21TRkXedUO"
      },
      "outputs": [],
      "source": [
        "train_generator = datagen.flow(training_patches, batch_size=int(training_patches.shape[0]/BATCH_SIZE), seed=1)\n",
        "train_label_generator = datagen.flow(training_patches_seg, batch_size=int(training_patches.shape[0]/BATCH_SIZE), seed=1)\n",
        "\n",
        "val_generator = datagen.flow(validation_patches, batch_size=int(validation_patches.shape[0]/BATCH_SIZE), seed=1)\n",
        "val_label_generator = datagen.flow(validation_patches_seg, batch_size=int(validation_patches.shape[0]/BATCH_SIZE), seed=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOw8_p4Oedag"
      },
      "outputs": [],
      "source": [
        "X_train = train_generator.next()\n",
        "y_train = train_label_generator.next()\n",
        "\n",
        "X_val = val_generator.next()\n",
        "y_val = val_label_generator.next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IELRGzM3r2mz",
        "outputId": "8c0ddaaa-ea47-4377-8872-2c634772b3d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11546, 32, 32, 1)\n",
            "(11546, 32, 32, 4)\n",
            "----------------\n",
            "(1201, 32, 32, 1)\n",
            "(1201, 32, 32, 4)\n"
          ]
        }
      ],
      "source": [
        "print(training_patches.shape)\n",
        "print(training_patches_seg.shape)\n",
        "print(\"----------------\")\n",
        "print(validation_patches.shape)\n",
        "print(validation_patches_seg.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce8mcx6Nr3QG",
        "outputId": "9ac9dedb-d128-4ab7-a46a-18691ff6105b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11726, 32, 32, 1)\n",
            "(11726, 32, 32, 4)\n",
            "(1219, 32, 32, 1)\n",
            "(1219, 32, 32, 4)\n"
          ]
        }
      ],
      "source": [
        "full_train = np.concatenate((training_patches, X_train))\n",
        "print(full_train.shape)\n",
        "full_train_label = np.concatenate((training_patches_seg, y_train))\n",
        "print(full_train_label.shape)\n",
        "\n",
        "full_val = np.concatenate((validation_patches, X_val))\n",
        "print(full_val.shape)\n",
        "full_val_label = np.concatenate((validation_patches_seg, y_val))\n",
        "print(full_val_label.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fdqvKEK1UES"
      },
      "source": [
        "##Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7H9hIPwEwJZ",
        "outputId": "eabbe778-74a8-4770-8f65-1f95df86e6cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "184/184 [==============================] - 23s 111ms/step - loss: 0.8389 - val_loss: 0.5091\n",
            "Epoch 2/200\n",
            "184/184 [==============================] - 20s 107ms/step - loss: 0.4302 - val_loss: 0.4703\n",
            "Epoch 3/200\n",
            "184/184 [==============================] - 20s 107ms/step - loss: 0.3568 - val_loss: 0.3959\n",
            "Epoch 4/200\n",
            "184/184 [==============================] - 20s 108ms/step - loss: 0.3116 - val_loss: 0.3814\n",
            "Epoch 5/200\n",
            "184/184 [==============================] - 20s 107ms/step - loss: 0.2743 - val_loss: 0.3527\n",
            "Epoch 6/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.2531 - val_loss: 0.3679\n",
            "Epoch 7/200\n",
            "184/184 [==============================] - 20s 107ms/step - loss: 0.2421 - val_loss: 0.3587\n",
            "Epoch 8/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.2237 - val_loss: 0.3675\n",
            "Epoch 9/200\n",
            "184/184 [==============================] - 20s 110ms/step - loss: 0.2125 - val_loss: 0.3306\n",
            "Epoch 10/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.2062 - val_loss: 0.3511\n",
            "Epoch 11/200\n",
            "184/184 [==============================] - 20s 107ms/step - loss: 0.1951 - val_loss: 0.3260\n",
            "Epoch 12/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1920 - val_loss: 0.3417\n",
            "Epoch 13/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1808 - val_loss: 0.3286\n",
            "Epoch 14/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1773 - val_loss: 0.3500\n",
            "Epoch 15/200\n",
            "184/184 [==============================] - 19s 104ms/step - loss: 0.1745 - val_loss: 0.3702\n",
            "Epoch 16/200\n",
            "184/184 [==============================] - 19s 104ms/step - loss: 0.1662 - val_loss: 0.3871\n",
            "Epoch 17/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1632 - val_loss: 0.3626\n",
            "Epoch 18/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1603 - val_loss: 0.3450\n",
            "Epoch 19/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1600 - val_loss: 0.3730\n",
            "Epoch 20/200\n",
            "184/184 [==============================] - 20s 107ms/step - loss: 0.1586 - val_loss: 0.3655\n",
            "Epoch 21/200\n",
            "184/184 [==============================] - 20s 108ms/step - loss: 0.1503 - val_loss: 0.3325\n",
            "Epoch 22/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1485 - val_loss: 0.3538\n",
            "Epoch 23/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1451 - val_loss: 0.3439\n",
            "Epoch 24/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1572 - val_loss: 0.3563\n",
            "Epoch 25/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1427 - val_loss: 0.3901\n",
            "Epoch 26/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1408 - val_loss: 0.3510\n",
            "Epoch 27/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1398 - val_loss: 0.3798\n",
            "Epoch 28/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1381 - val_loss: 0.3544\n",
            "Epoch 29/200\n",
            "184/184 [==============================] - 20s 108ms/step - loss: 0.1378 - val_loss: 0.3638\n",
            "Epoch 30/200\n",
            "184/184 [==============================] - 20s 108ms/step - loss: 0.1371 - val_loss: 0.3176\n",
            "Epoch 31/200\n",
            "184/184 [==============================] - 20s 108ms/step - loss: 0.1346 - val_loss: 0.2990\n",
            "Epoch 32/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1326 - val_loss: 0.3477\n",
            "Epoch 33/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1333 - val_loss: 0.3590\n",
            "Epoch 34/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1297 - val_loss: 0.4139\n",
            "Epoch 35/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1283 - val_loss: 0.3351\n",
            "Epoch 36/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1290 - val_loss: 0.3584\n",
            "Epoch 37/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1263 - val_loss: 0.3625\n",
            "Epoch 38/200\n",
            "184/184 [==============================] - 20s 108ms/step - loss: 0.1278 - val_loss: 0.3729\n",
            "Epoch 39/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1253 - val_loss: 0.3278\n",
            "Epoch 40/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1253 - val_loss: 0.3438\n",
            "Epoch 41/200\n",
            "184/184 [==============================] - 19s 104ms/step - loss: 0.1233 - val_loss: 0.3334\n",
            "Epoch 42/200\n",
            "184/184 [==============================] - 19s 104ms/step - loss: 0.1229 - val_loss: 0.3701\n",
            "Epoch 43/200\n",
            "184/184 [==============================] - 19s 104ms/step - loss: 0.1210 - val_loss: 0.4197\n",
            "Epoch 44/200\n",
            "184/184 [==============================] - 19s 104ms/step - loss: 0.1245 - val_loss: 0.3637\n",
            "Epoch 45/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1213 - val_loss: 0.4220\n",
            "Epoch 46/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1230 - val_loss: 0.3549\n",
            "Epoch 47/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1188 - val_loss: 0.3710\n",
            "Epoch 48/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1199 - val_loss: 0.3927\n",
            "Epoch 49/200\n",
            "184/184 [==============================] - 20s 108ms/step - loss: 0.1188 - val_loss: 0.3995\n",
            "Epoch 50/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1177 - val_loss: 0.4288\n",
            "Epoch 51/200\n",
            "184/184 [==============================] - 19s 105ms/step - loss: 0.1170 - val_loss: 0.3758\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f250ebd17d0>"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ],
      "source": [
        "my_callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=PATIENCE), # early stopping\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath=MODEL_FNAME_PATTERN, save_best_only=True) # save the best based on validation\n",
        "]\n",
        "\n",
        "unet = get_unet()\n",
        "unet.compile(optimizer=OPTIMISER, loss=LOSS)\n",
        "unet.fit(\n",
        "    x=full_train, \n",
        "    y=full_train_label,\n",
        "    validation_data=(full_val, full_val_label),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=N_EPOCHS,\n",
        "    callbacks=my_callbacks,\n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGcfrZCMjz4M"
      },
      "source": [
        "##Load the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fNsYJL7KnON"
      },
      "outputs": [],
      "source": [
        "unet = get_unet(\n",
        "    img_size=(IMAGE_SIZE[1], IMAGE_SIZE[2]),\n",
        "    n_classes=N_CLASSES,\n",
        "    n_input_channels=N_INPUT_CHANNELS)\n",
        "unet.compile(optimizer=OPTIMISER, loss=LOSS)\n",
        "unet.load_weights('model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwPzN0jrSm8r"
      },
      "source": [
        "##Prepare test data using the validation volumes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuRHLNYGkDFp"
      },
      "outputs": [],
      "source": [
        "def prepare_val_data(the_volumes, the_labels):\n",
        "  testing_volumes_processed = the_volumes.reshape([-1, IMAGE_SIZE[1], IMAGE_SIZE[2], 1])\n",
        "  testing_labels_processed = the_labels.reshape([-1, IMAGE_SIZE[1], IMAGE_SIZE[2], 1])\n",
        "\n",
        "  testing_labels_processed = tf.keras.utils.to_categorical(testing_labels_processed, num_classes=4, dtype='float32')\n",
        "\n",
        "  #print(testing_volumes_processed.shape)\n",
        "  #print(testing_labels_processed.shape)\n",
        "\n",
        "  return (testing_volumes_processed, testing_labels_processed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbJZUQyLqF4p"
      },
      "source": [
        "###Predict labels for test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItoY31x0K3r8"
      },
      "outputs": [],
      "source": [
        "def pred_val_data(testing_volumes_processed)  :\n",
        "  # creates probability map of each label for all volumes\n",
        "  prediction = unet.predict(x=testing_volumes_processed)\n",
        "\n",
        "  prediction = np.argmax(prediction, axis=3)\n",
        "\n",
        "  #plt.axis('off')\n",
        "  #plt.imshow(prediction[:, :, 150])\n",
        "\n",
        "  return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VU2XAj_wxgJv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f761f4c7-6a05-480b-bd07-b0fcf8b502bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nprint(prediction.shape)\\nprint(testing_labels_processed.shape)\\nprint(testing_volumes_T1_processed.shape)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "\"\"\"\n",
        "print(prediction.shape)\n",
        "print(testing_labels_processed.shape)\n",
        "print(testing_volumes_T1_processed.shape)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Computing Dice, AVD and HD (Final)\n",
        "\n"
      ],
      "metadata": {
        "id": "5RHaF_LhpWXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_hausdorff_distance(in1, in2, label = 'all'):\n",
        "    in1=in1.squeeze()\n",
        "    in2=in2.squeeze()\n",
        "    hausdorff_distance_filter = sitk.HausdorffDistanceImageFilter()\n",
        "    if label == 'all':\n",
        "        # Hausdorff distance\n",
        "        hausdorff_distance_filter.Execute(in1, in2)\n",
        "    else:\n",
        "        in1_array  = in1 #sitk.GetArrayFromImage(in1)\n",
        "        in1_array = (in1_array == label) *1 \n",
        "        in1_array = in1_array.astype('uint16')  \n",
        "        img1 = sitk.GetImageFromArray(in1_array)\n",
        "        \n",
        "        in2_array  = in2 #sitk.GetArrayFromImage(in2)\n",
        "        in2_array = (in2_array == label) *1 \n",
        "        in2_array = in2_array.astype('uint16')  \n",
        "        img2 = sitk.GetImageFromArray(in2_array)\n",
        "        # Hausdorff distance\n",
        "        hausdorff_distance_filter.Execute(img1, img2)\n",
        "    return hausdorff_distance_filter.GetHausdorffDistance()\n",
        "\n",
        "def compute_dice_coefficient(in1, in2, label  = 'all'):\n",
        "    in1=in1.squeeze()\n",
        "    in2=in2.squeeze()\n",
        "    if label=='all': \n",
        "        return 2 * np.sum( (in1>0) &  (in2>0) & (in1 == in2)) / (np.sum(in1 > 0) + np.sum(in2 > 0))\n",
        "    else:\n",
        "        return 2 * np.sum((in1 == label) & (in2 == label)) / (np.sum(in1 == label) + np.sum(in2 == label))\n",
        "\n",
        "def compute_volumentric_difference(in1, in2, label  = 'all'):\n",
        "    in1=in1.squeeze()\n",
        "    in2=in2.squeeze()\n",
        "    if label  == 'all':\n",
        "      #  vol_dif  = np.sum((in1 != in2) & (in1 !=0) & (in2 !=0))\n",
        "        return np.sum((in1 != in2)) / ((np.sum(in1 > 0) + np.sum(in2 > 0)))\n",
        "    else:\n",
        "        in1  = (in1 == label) * 1\n",
        "        in2  = (in2 == label) * 1\n",
        "        return np.sum((in1 != in2)) / ((np.sum(in1 > 0) + np.sum(in2 > 0)))"
      ],
      "metadata": {
        "id": "iFBnx17Bpa_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cl in range(0,4,1): \n",
        "  overallDSC = np.zeros(N_CLASSES)\n",
        "  overall_Hausdorff = np.zeros(N_CLASSES)\n",
        "  overall_vol = np.zeros(N_CLASSES)\n",
        "\n",
        "  for i in range(0,validation_volumes.shape[0], 1):\n",
        "      \n",
        "      testing_volumes_processed, testing_labels_processed = prepare_val_data(v_volumes[i], v_labels[i])\n",
        "      prediction = pred_val_data(testing_volumes_processed)\n",
        "      \n",
        "      #cl = 3\n",
        "\n",
        "      cur_DSC = compute_dice_coefficient(prediction, v_labels[i], label=cl)\n",
        "      overallDSC = overallDSC + cur_DSC\n",
        "\n",
        "      cur_Hausdorff = compute_hausdorff_distance(prediction, v_labels[i], label=cl) \n",
        "      overall_Hausdorff = overall_Hausdorff + cur_Hausdorff\n",
        "\n",
        "      cur_vol = compute_volumentric_difference(prediction, v_labels[i], label=cl)\n",
        "      overall_vol = overall_vol + cur_vol\n",
        "      \n",
        "      #print(prediction.shape)\n",
        "      #print(v_labels[i].shape)\n",
        "      \n",
        "  #print(overall_Hausdorff)\n",
        "  overallDSC = overallDSC/validation_volumes.shape[0]\n",
        "  overall_Hausdorff = overall_Hausdorff/validation_volumes.shape[0]\n",
        "  overall_vol = overall_vol/validation_volumes.shape[0]\n",
        "\n",
        "  # for i in range(0,cl,1):\n",
        "  #print(\"Class {} - Dice Coefficient = {:.4f}\".format(cl, overallDSC[i]))\n",
        "  #print(\"Class {} - HD = {:.4f}\".format(cl, overall_Hausdorff[i]))\n",
        "  #print(\"Class {} - AVD = {:.4f}\".format(cl, overall_vol[i]))\n",
        "  print(\"Class {}\".format(cl))\n",
        "  print(\"\\tDice Coefficient = {:.4f}\".format(overallDSC[i]))\n",
        "  # print(\"\\tHD = {:.4f}\".format(overall_Hausdorff[i]))\n",
        "  # print(\"\\tAVD = {:.4f}\".format(overall_vol[i]))"
      ],
      "metadata": {
        "id": "8PrStjpepejb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3e28d78-a4ef-4150-bb88-87817cca661f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0\n",
            "\tDice Coefficient = 0.9975\n",
            "Class 1\n",
            "\tDice Coefficient = 0.8130\n",
            "Class 2\n",
            "\tDice Coefficient = 0.9055\n",
            "Class 3\n",
            "\tDice Coefficient = 0.8355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3zTHEeiN4kE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "67bdf16e-2415-4d41-bfce-1c76110ac374"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nClass 0 - Dice Coefficient 0.9975\\nClass 1 - Dice Coefficient 0.8342\\nClass 2 - Dice Coefficient 0.9209\\nClass 3 - Dice Coefficient 0.8825\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ],
      "source": [
        "# 2DUNet\n",
        "# batch size = 32, patient = 5, dropout=0.15, epoch = 50\n",
        "\"\"\"\n",
        "Class 0 - Dice Coefficient 0.9976\n",
        "Class 1 - Dice Coefficient 0.8288\n",
        "Class 2 - Dice Coefficient 0.9186\n",
        "Class 3 - Dice Coefficient 0.8765\n",
        "\"\"\"\n",
        "\n",
        "# batch size = 40, patient = 5, dropout=0.15, epoch = 50\n",
        "\"\"\"\n",
        "Class 0 - Dice Coefficient 0.9977\n",
        "Class 1 - Dice Coefficient 0.8261\n",
        "Class 2 - Dice Coefficient 0.9202\n",
        "Class 3 - Dice Coefficient 0.8790\n",
        "\"\"\"\n",
        "\n",
        "# batch size = 50, patient = 20, dropout=0.15, epoch = 200\n",
        "\"\"\"\n",
        "Class 0 - Dice Coefficient 0.9977\n",
        "Class 1 - Dice Coefficient 0.8261\n",
        "Class 2 - Dice Coefficient 0.9202\n",
        "Class 3 - Dice Coefficient 0.8790\n",
        "\"\"\"\n",
        "\n",
        "# batch size = 64, patient = 20, dropout=0.40, epoch = 200\n",
        "\"\"\"\n",
        "Class 0 - Dice Coefficient 0.9975\n",
        "Class 1 - Dice Coefficient 0.8342\n",
        "Class 2 - Dice Coefficient 0.9209\n",
        "Class 3 - Dice Coefficient 0.8825\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CipR5ZtUqSDC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ByBoNkPGWKrN",
        "uZ2BevUuIkGs"
      ],
      "name": "2DResUNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}